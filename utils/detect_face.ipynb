{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# OpenCV is optional, but required if using numpy arrays instead of PIL\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def fixed_batch_process(im_data, model):\n",
    "    batch_size = 512\n",
    "    out = []\n",
    "    for i in range(0, len(im_data), batch_size):\n",
    "        batch = im_data[i:(i+batch_size)]\n",
    "        out.append(model(batch))\n",
    "\n",
    "    return tuple(torch.cat(v, dim=0) for v in zip(*out))\n",
    "\n",
    "def detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):\n",
    "    if isinstance(imgs, (np.ndarray, torch.Tensor)):\n",
    "        if isinstance(imgs,np.ndarray):\n",
    "            imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "        if isinstance(imgs,torch.Tensor):\n",
    "            imgs = torch.as_tensor(imgs, device=device)\n",
    "\n",
    "        if len(imgs.shape) == 3:\n",
    "            imgs = imgs.unsqueeze(0)\n",
    "    else:\n",
    "        if not isinstance(imgs, (list, tuple)):\n",
    "            imgs = [imgs]\n",
    "        if any(img.size != imgs[0].size for img in imgs):\n",
    "            raise Exception(\"MTCNN batch processing only compatible with equal-dimension images.\")\n",
    "        imgs = np.stack([np.uint8(img) for img in imgs])\n",
    "        imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "    \n",
    "\n",
    "    model_dtype = next(pnet.parameters()).dtype\n",
    "    imgs = imgs.permute(0, 3, 1, 2).type(model_dtype)\n",
    "\n",
    "    batch_size = len(imgs)\n",
    "    h, w = imgs.shape[2:4]\n",
    "    m = 12.0 / minsize\n",
    "    minl = min(h, w)\n",
    "    minl = minl * m\n",
    "\n",
    "    # Create scale pyramid\n",
    "    scale_i = m\n",
    "    scales = []\n",
    "    while minl >= 12:\n",
    "        scales.append(scale_i)\n",
    "        scale_i = scale_i * factor\n",
    "        minl = minl * factor\n",
    "\n",
    "    # First stage\n",
    "    boxes = []\n",
    "    image_inds = []\n",
    "\n",
    "    scale_picks = []\n",
    "\n",
    "    all_i = 0\n",
    "    offset = 0\n",
    "    for scale in scales:\n",
    "        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        reg, probs = pnet(im_data)\n",
    "    \n",
    "        boxes_scale, image_inds_scale = generateBoundingBox(reg, probs[:, 1], scale, threshold[0])\n",
    "        boxes.append(boxes_scale)\n",
    "        image_inds.append(image_inds_scale)\n",
    "\n",
    "        pick = batched_nms(boxes_scale[:, :4], boxes_scale[:, 4], image_inds_scale, 0.5)\n",
    "        scale_picks.append(pick + offset)\n",
    "        offset += boxes_scale.shape[0]\n",
    "\n",
    "    boxes = torch.cat(boxes, dim=0)\n",
    "    image_inds = torch.cat(image_inds, dim=0)\n",
    "\n",
    "    scale_picks = torch.cat(scale_picks, dim=0)\n",
    "\n",
    "    # NMS within each scale + image\n",
    "    boxes, image_inds = boxes[scale_picks], image_inds[scale_picks]\n",
    "\n",
    "\n",
    "    # NMS within each image\n",
    "    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "    boxes, image_inds = boxes[pick], image_inds[pick]\n",
    "\n",
    "    regw = boxes[:, 2] - boxes[:, 0]\n",
    "    regh = boxes[:, 3] - boxes[:, 1]\n",
    "    qq1 = boxes[:, 0] + boxes[:, 5] * regw\n",
    "    qq2 = boxes[:, 1] + boxes[:, 6] * regh\n",
    "    qq3 = boxes[:, 2] + boxes[:, 7] * regw\n",
    "    qq4 = boxes[:, 3] + boxes[:, 8] * regh\n",
    "    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)\n",
    "    boxes = rerec(boxes)\n",
    "    y, ey, x, ex = pad(boxes, w, h)\n",
    "    \n",
    "    # Second stage\n",
    "    if len(boxes) > 0:\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (24, 24)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "\n",
    "        # This is equivalent to out = rnet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, rnet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        score = out1[1, :]\n",
    "        ipass = score > threshold[1]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        # NMS within each image\n",
    "        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]\n",
    "        boxes = bbreg(boxes, mv)\n",
    "        boxes = rerec(boxes)\n",
    "\n",
    "    # Third stage\n",
    "    points = torch.zeros(0, 5, 2, device=device)\n",
    "    if len(boxes) > 0:\n",
    "        y, ey, x, ex = pad(boxes, w, h)\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (48, 48)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        \n",
    "        # This is equivalent to out = onet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, onet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        out2 = out[2].permute(1, 0)\n",
    "        score = out2[1, :]\n",
    "        points = out1\n",
    "        ipass = score > threshold[2]\n",
    "        points = points[:, ipass]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        w_i = boxes[:, 2] - boxes[:, 0] + 1\n",
    "        h_i = boxes[:, 3] - boxes[:, 1] + 1\n",
    "        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1\n",
    "        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1\n",
    "        points = torch.stack((points_x, points_y)).permute(2, 1, 0)\n",
    "        boxes = bbreg(boxes, mv)\n",
    "\n",
    "        # NMS within each image using \"Min\" strategy\n",
    "        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, 'Min')\n",
    "        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]\n",
    "\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    points = points.cpu().numpy()\n",
    "\n",
    "    image_inds = image_inds.cpu()\n",
    "\n",
    "    batch_boxes = []\n",
    "    batch_points = []\n",
    "    for b_i in range(batch_size):\n",
    "        b_i_inds = np.where(image_inds == b_i)\n",
    "        batch_boxes.append(boxes[b_i_inds].copy())\n",
    "        batch_points.append(points[b_i_inds].copy())\n",
    "\n",
    "    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
    "\n",
    "    return batch_boxes, batch_points\n",
    "\n",
    "\n",
    "def bbreg(boundingbox, reg):\n",
    "    if reg.shape[1] == 1:\n",
    "        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))\n",
    "\n",
    "    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n",
    "    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n",
    "    b1 = boundingbox[:, 0] + reg[:, 0] * w\n",
    "    b2 = boundingbox[:, 1] + reg[:, 1] * h\n",
    "    b3 = boundingbox[:, 2] + reg[:, 2] * w\n",
    "    b4 = boundingbox[:, 3] + reg[:, 3] * h\n",
    "    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)\n",
    "\n",
    "    return boundingbox\n",
    "\n",
    "\n",
    "def generateBoundingBox(reg, probs, scale, thresh):\n",
    "    stride = 2\n",
    "    cellsize = 12\n",
    "\n",
    "    reg = reg.permute(1, 0, 2, 3)\n",
    "\n",
    "    mask = probs >= thresh\n",
    "    mask_inds = mask.nonzero()\n",
    "    image_inds = mask_inds[:, 0]\n",
    "    score = probs[mask]\n",
    "    reg = reg[:, mask].permute(1, 0)\n",
    "    bb = mask_inds[:, 1:].type(reg.dtype).flip(1)\n",
    "    q1 = ((stride * bb + 1) / scale).floor()\n",
    "    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()\n",
    "    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)\n",
    "    return boundingbox, image_inds\n",
    "\n",
    "\n",
    "def nms_numpy(boxes, scores, threshold, method):\n",
    "    if boxes.size == 0:\n",
    "        return np.empty((0, 3))\n",
    "\n",
    "    x1 = boxes[:, 0].copy()\n",
    "    y1 = boxes[:, 1].copy()\n",
    "    x2 = boxes[:, 2].copy()\n",
    "    y2 = boxes[:, 3].copy()\n",
    "    s = scores\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    I = np.argsort(s)\n",
    "    pick = np.zeros_like(s, dtype=np.int16)\n",
    "    counter = 0\n",
    "    while I.size > 0:\n",
    "        i = I[-1]\n",
    "        pick[counter] = i\n",
    "        counter += 1\n",
    "        idx = I[0:-1]\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idx]).copy()\n",
    "        yy1 = np.maximum(y1[i], y1[idx]).copy()\n",
    "        xx2 = np.minimum(x2[i], x2[idx]).copy()\n",
    "        yy2 = np.minimum(y2[i], y2[idx]).copy()\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1).copy()\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1).copy()\n",
    "\n",
    "        inter = w * h\n",
    "        if method is \"Min\":\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "        I = I[np.where(o <= threshold)]\n",
    "\n",
    "    pick = pick[:counter].copy()\n",
    "    return pick\n",
    "\n",
    "\n",
    "def batched_nms_numpy(boxes, scores, idxs, threshold, method):\n",
    "    device = boxes.device\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=device)\n",
    "    # strategy: in order to perform NMS independently per class.\n",
    "    # we add an offset to all the boxes. The offset is dependent\n",
    "    # only on the class idx, and is large enough so that boxes\n",
    "    # from different classes do not overlap\n",
    "    max_coordinate = boxes.max()\n",
    "    offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "    boxes_for_nms = boxes + offsets[:, None]\n",
    "    boxes_for_nms = boxes_for_nms.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "    keep = nms_numpy(boxes_for_nms, scores, threshold, method)\n",
    "    return torch.as_tensor(keep, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "def pad(boxes, w, h):\n",
    "    boxes = boxes.trunc().int().cpu().numpy()\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    ex = boxes[:, 2]\n",
    "    ey = boxes[:, 3]\n",
    "\n",
    "    x[x < 1] = 1\n",
    "    y[y < 1] = 1\n",
    "    ex[ex > w] = w\n",
    "    ey[ey > h] = h\n",
    "\n",
    "    return y, ey, x, ex\n",
    "\n",
    "\n",
    "def rerec(bboxA):\n",
    "    h = bboxA[:, 3] - bboxA[:, 1]\n",
    "    w = bboxA[:, 2] - bboxA[:, 0]\n",
    "    \n",
    "    l = torch.max(w, h)\n",
    "    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n",
    "    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n",
    "    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)\n",
    "\n",
    "    return bboxA\n",
    "\n",
    "\n",
    "def imresample(img, sz):\n",
    "    im_data = interpolate(img, size=sz, mode=\"area\")\n",
    "    return im_data\n",
    "\n",
    "\n",
    "def crop_resize(img, box, image_size):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = cv2.resize(\n",
    "            img,\n",
    "            (image_size, image_size),\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        ).copy()\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = imresample(\n",
    "            img.permute(2, 0, 1).unsqueeze(0).float(),\n",
    "            (image_size, image_size)\n",
    "        ).byte().squeeze(0).permute(1, 2, 0)\n",
    "    else:\n",
    "        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_img(img, path):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    else:\n",
    "        img.save(path)\n",
    "\n",
    "\n",
    "def get_size(img):\n",
    "    if isinstance(img, (np.ndarray, torch.Tensor)):\n",
    "        return img.shape[1::-1]\n",
    "    else:\n",
    "        return img.size\n",
    "\n",
    "\n",
    "def extract_face(img, box, image_size=160, margin=0, save_path=None):\n",
    "    \"\"\"Extract face + margin from PIL Image given bounding box.\n",
    "    \n",
    "    Arguments:\n",
    "        img {PIL.Image} -- A PIL Image.\n",
    "        box {numpy.ndarray} -- Four-element bounding box.\n",
    "        image_size {int} -- Output image size in pixels. The image will be square.\n",
    "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
    "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
    "            repo, which applies the margin to the original image before resizing, making the margin\n",
    "            dependent on the original image size.\n",
    "        save_path {str} -- Save path for extracted face image. (default: {None})\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- tensor representing the extracted face.\n",
    "    \"\"\"\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (image_size - margin),\n",
    "        margin * (box[3] - box[1]) / (image_size - margin),\n",
    "    ]\n",
    "    raw_image_size = get_size(img)\n",
    "    box = [\n",
    "        int(max(box[0] - margin[0] / 2, 0)),\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\n",
    "        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\n",
    "    ]\n",
    "\n",
    "    face = crop_resize(img, box, image_size)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path) + \"/\", exist_ok=True)\n",
    "        save_img(face, save_path)\n",
    "\n",
    "    face = F.to_tensor(np.float32(face))\n",
    "\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
