{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-66c187bfd644>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-66c187bfd644>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    python3 -m utils.detect_face\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# from .utils.detect_face import detect_face, extract_face\n",
    "python3 -m utils.detect_face\n",
    "\n",
    "\n",
    "class PNet(nn.Module):\n",
    "    \"\"\"MTCNN PNet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(10)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.prelu3 = nn.PReLU(32)\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)\n",
    "        self.softmax4_1 = nn.Softmax(dim=1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/pnet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        a = self.conv4_1(x)\n",
    "        a = self.softmax4_1(a)\n",
    "        b = self.conv4_2(x)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class RNet(nn.Module):\n",
    "    \"\"\"MTCNN RNet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 28, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(28)\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(28, 48, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(48)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv3 = nn.Conv2d(48, 64, kernel_size=2)\n",
    "        self.prelu3 = nn.PReLU(64)\n",
    "        self.dense4 = nn.Linear(576, 128)\n",
    "        self.prelu4 = nn.PReLU(128)\n",
    "        self.dense5_1 = nn.Linear(128, 2)\n",
    "        self.softmax5_1 = nn.Softmax(dim=1)\n",
    "        self.dense5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/rnet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.dense4(x.view(x.shape[0], -1))\n",
    "        x = self.prelu4(x)\n",
    "        a = self.dense5_1(x)\n",
    "        a = self.softmax5_1(a)\n",
    "        b = self.dense5_2(x)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class ONet(nn.Module):\n",
    "    \"\"\"MTCNN ONet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(32)\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(64)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.prelu3 = nn.PReLU(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.prelu4 = nn.PReLU(128)\n",
    "        self.dense5 = nn.Linear(1152, 256)\n",
    "        self.prelu5 = nn.PReLU(256)\n",
    "        self.dense6_1 = nn.Linear(256, 2)\n",
    "        self.softmax6_1 = nn.Softmax(dim=1)\n",
    "        self.dense6_2 = nn.Linear(256, 4)\n",
    "        self.dense6_3 = nn.Linear(256, 10)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/onet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.dense5(x.view(x.shape[0], -1))\n",
    "        x = self.prelu5(x)\n",
    "        a = self.dense6_1(x)\n",
    "        a = self.softmax6_1(a)\n",
    "        b = self.dense6_2(x)\n",
    "        c = self.dense6_3(x)\n",
    "        return b, c, a\n",
    "\n",
    "\n",
    "class MTCNN(nn.Module):\n",
    "    \"\"\"MTCNN face detection module.\n",
    "    This class loads pretrained P-, R-, and O-nets and returns images cropped to include the face\n",
    "    only, given raw input images of one of the following types:\n",
    "        - PIL image or list of PIL images\n",
    "        - numpy.ndarray (uint8) representing either a single image (3D) or a batch of images (4D).\n",
    "    Cropped faces can optionally be saved to file\n",
    "    also.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\n",
    "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
    "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
    "            repo, which applies the margin to the original image before resizing, making the margin\n",
    "            dependent on the original image size (this is a bug in davidsandberg/facenet).\n",
    "            (default: {0})\n",
    "        min_face_size {int} -- Minimum face size to search for. (default: {20})\n",
    "        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\n",
    "        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\n",
    "        post_process {bool} -- Whether or not to post process images tensors before returning.\n",
    "            (default: {True})\n",
    "        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\n",
    "            If False, the face with the highest detection probability is returned.\n",
    "            (default: {True})\n",
    "        selection_method {string} -- Which heuristic to use for selection. Default None. If\n",
    "            specified, will override select_largest:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability selected\n",
    "                    \"center_weighted_size\": box size minus weighted squared offset from image center\n",
    "                (default: {None})\n",
    "        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\n",
    "            select_largest parameter. If a save_path is specified, the first face is saved to that\n",
    "            path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\n",
    "            (default: {False})\n",
    "        device {torch.device} -- The device on which to run neural net passes. Image tensors and\n",
    "            models are copied to this device before running forward passes. (default: {None})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, image_size=160, margin=0, min_face_size=20,\n",
    "        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "        select_largest=True, selection_method=None, keep_all=False, device=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.margin = margin\n",
    "        self.min_face_size = min_face_size\n",
    "        self.thresholds = thresholds\n",
    "        self.factor = factor\n",
    "        self.post_process = post_process\n",
    "        self.select_largest = select_largest\n",
    "        self.keep_all = keep_all\n",
    "        self.selection_method = selection_method\n",
    "\n",
    "        self.pnet = PNet()\n",
    "        self.rnet = RNet()\n",
    "        self.onet = ONet()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "            self.to(device)\n",
    "\n",
    "        if not self.selection_method:\n",
    "            self.selection_method = 'largest' if self.select_largest else 'probability'\n",
    "\n",
    "    def forward(self, img, save_path=None, return_prob=False):\n",
    "        \"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\n",
    "        detection and extraction of faces, returning tensors representing detected faces rather\n",
    "        than the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\n",
    "        \n",
    "        Arguments:\n",
    "            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            save_path {str} -- An optional save path for the cropped image. Note that when\n",
    "                self.post_process=True, although the returned tensor is post processed, the saved\n",
    "                face image is not, so it is a true representation of the face in the input image.\n",
    "                If `img` is a list of images, `save_path` should be a list of equal length.\n",
    "                (default: {None})\n",
    "            return_prob {bool} -- Whether or not to return the detection probability.\n",
    "                (default: {False})\n",
    "        \n",
    "        Returns:\n",
    "            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\n",
    "                with dimensions 3 x image_size x image_size. Optionally, the probability that a\n",
    "                face was detected. If self.keep_all is True, n detected faces are returned in an\n",
    "                n x 3 x image_size x image_size tensor with an optional list of detection\n",
    "                probabilities. If `img` is a list of images, the item(s) returned have an extra \n",
    "                dimension (batch) as the first dimension.\n",
    "        Example:\n",
    "        >>> from facenet_pytorch import MTCNN\n",
    "        >>> mtcnn = MTCNN()\n",
    "        >>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\n",
    "        \"\"\"\n",
    "\n",
    "        # Detect faces\n",
    "        batch_boxes, batch_probs, batch_points = self.detect(img, landmarks=True)\n",
    "        # Select faces\n",
    "        if not self.keep_all:\n",
    "            batch_boxes, batch_probs, batch_points = self.select_boxes(\n",
    "                batch_boxes, batch_probs, batch_points, img, method=self.selection_method\n",
    "            )\n",
    "        # Extract faces\n",
    "        faces = self.extract(img, batch_boxes, save_path)\n",
    "\n",
    "        if return_prob:\n",
    "            return faces, batch_probs\n",
    "        else:\n",
    "            return faces\n",
    "\n",
    "    def detect(self, img, landmarks=False):\n",
    "        \"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\n",
    "        This method is used by the forward method and is also useful for face detection tasks\n",
    "        that require lower-level handling of bounding boxes and facial landmarks (e.g., face\n",
    "        tracking). The functionality of the forward function can be emulated by using this method\n",
    "        followed by the extract_face() function.\n",
    "        \n",
    "        Arguments:\n",
    "            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        Keyword Arguments:\n",
    "            landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.\n",
    "                (default: {False})\n",
    "        \n",
    "        Returns:\n",
    "            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\n",
    "                Nx4 array of bounding boxes and a length N list of detection probabilities.\n",
    "                Returned boxes will be sorted in descending order by detection probability if\n",
    "                self.select_largest=False, otherwise the largest face will be returned first.\n",
    "                If `img` is a list of images, the items returned have an extra dimension\n",
    "                (batch) as the first dimension. Optionally, a third item, the facial landmarks,\n",
    "                are returned if `landmarks=True`.\n",
    "        Example:\n",
    "        >>> from PIL import Image, ImageDraw\n",
    "        >>> from facenet_pytorch import MTCNN, extract_face\n",
    "        >>> mtcnn = MTCNN(keep_all=True)\n",
    "        >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
    "        >>> # Draw boxes and save faces\n",
    "        >>> img_draw = img.copy()\n",
    "        >>> draw = ImageDraw.Draw(img_draw)\n",
    "        >>> for i, (box, point) in enumerate(zip(boxes, points)):\n",
    "        ...     draw.rectangle(box.tolist(), width=5)\n",
    "        ...     for p in point:\n",
    "        ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\n",
    "        ...     extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "        >>> img_draw.save('annotated_faces.png')\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_boxes, batch_points = detect_face(\n",
    "                img, self.min_face_size,\n",
    "                self.pnet, self.rnet, self.onet,\n",
    "                self.thresholds, self.factor,\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "        boxes, probs, points = [], [], []\n",
    "        for box, point in zip(batch_boxes, batch_points):\n",
    "            box = np.array(box)\n",
    "            point = np.array(point)\n",
    "            if len(box) == 0:\n",
    "                boxes.append(None)\n",
    "                probs.append([None])\n",
    "                points.append(None)\n",
    "            elif self.select_largest:\n",
    "                box_order = np.argsort((box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1]))[::-1]\n",
    "                box = box[box_order]\n",
    "                point = point[box_order]\n",
    "                boxes.append(box[:, :4])\n",
    "                probs.append(box[:, 4])\n",
    "                points.append(point)\n",
    "            else:\n",
    "                boxes.append(box[:, :4])\n",
    "                probs.append(box[:, 4])\n",
    "                points.append(point)\n",
    "        boxes = np.array(boxes)\n",
    "        probs = np.array(probs)\n",
    "        points = np.array(points)\n",
    "\n",
    "        if (\n",
    "            not isinstance(img, (list, tuple)) and \n",
    "            not (isinstance(img, np.ndarray) and len(img.shape) == 4) and\n",
    "            not (isinstance(img, torch.Tensor) and len(img.shape) == 4)\n",
    "        ):\n",
    "            boxes = boxes[0]\n",
    "            probs = probs[0]\n",
    "            points = points[0]\n",
    "\n",
    "        if landmarks:\n",
    "            return boxes, probs, points\n",
    "\n",
    "        return boxes, probs\n",
    "\n",
    "    def select_boxes(\n",
    "        self, all_boxes, all_probs, all_points, imgs, method='probability', threshold=0.9,\n",
    "        center_weight=2.0\n",
    "    ):\n",
    "        \"\"\"Selects a single box from multiple for a given image using one of multiple heuristics.\n",
    "        Arguments:\n",
    "                all_boxes {np.ndarray} -- Ix0 ndarray where each element is a Nx4 ndarry of\n",
    "                    bounding boxes for N detected faces in I images (output from self.detect).\n",
    "                all_probs {np.ndarray} -- Ix0 ndarray where each element is a Nx0 ndarry of\n",
    "                    probabilities for N detected faces in I images (output from self.detect).\n",
    "                all_points {np.ndarray} -- Ix0 ndarray where each element is a Nx5x2 array of\n",
    "                    points for N detected faces. (output from self.detect).\n",
    "                imgs {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        Keyword Arguments:\n",
    "                method {str} -- Which heuristic to use for selection:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability selected\n",
    "                    \"center_weighted_size\": box size minus weighted squared offset from image center\n",
    "                    (default: {'probability'})\n",
    "                threshold {float} -- theshold for \"largest_over_threshold\" method. (default: {0.9})\n",
    "                center_weight {float} -- weight for squared offset in center weighted size method.\n",
    "                    (default: {2.0})\n",
    "        Returns:\n",
    "                tuple(numpy.ndarray, numpy.ndarray, numpy.ndarray) -- nx4 ndarray of bounding boxes\n",
    "                    for n images. Ix0 array of probabilities for each box, array of landmark points.\n",
    "        \"\"\"\n",
    "\n",
    "        #copying batch detection from extract, but would be easier to ensure detect creates consistent output.\n",
    "        batch_mode = True\n",
    "        if (\n",
    "                not isinstance(imgs, (list, tuple)) and\n",
    "                not (isinstance(imgs, np.ndarray) and len(imgs.shape) == 4) and\n",
    "                not (isinstance(imgs, torch.Tensor) and len(imgs.shape) == 4)\n",
    "        ):\n",
    "            imgs = [imgs]\n",
    "            all_boxes = [all_boxes]\n",
    "            all_probs = [all_probs]\n",
    "            all_points = [all_points]\n",
    "            batch_mode = False\n",
    "\n",
    "        selected_boxes, selected_probs, selected_points = [], [], []\n",
    "        for boxes, points, probs, img in zip(all_boxes, all_points, all_probs, imgs):\n",
    "            \n",
    "            if boxes is None:\n",
    "                selected_boxes.append(None)\n",
    "                selected_probs.append([None])\n",
    "                selected_points.append(None)\n",
    "                continue\n",
    "            \n",
    "            # If at least 1 box found\n",
    "            boxes = np.array(boxes)\n",
    "            probs = np.array(probs)\n",
    "            points = np.array(points)\n",
    "                \n",
    "            if method == 'largest':\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "            elif method == 'probability':\n",
    "                box_order = np.argsort(probs)[::-1]\n",
    "            elif method == 'center_weighted_size':\n",
    "                box_sizes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "                img_center = (img.width / 2, img.height/2)\n",
    "                box_centers = np.array(list(zip((boxes[:, 0] + boxes[:, 2]) / 2, (boxes[:, 1] + boxes[:, 3]) / 2)))\n",
    "                offsets = box_centers - img_center\n",
    "                offset_dist_squared = np.sum(np.power(offsets, 2.0), 1)\n",
    "                box_order = np.argsort(box_sizes - offset_dist_squared * center_weight)[::-1]\n",
    "            elif method == 'largest_over_threshold':\n",
    "                box_mask = probs > threshold\n",
    "                boxes = boxes[box_mask]\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "                if sum(box_mask) == 0:\n",
    "                    selected_boxes.append(None)\n",
    "                    selected_probs.append([None])\n",
    "                    selected_points.append(None)\n",
    "                    continue\n",
    "\n",
    "            box = boxes[box_order][[0]]\n",
    "            prob = probs[box_order][[0]]\n",
    "            point = points[box_order][[0]]\n",
    "            selected_boxes.append(box)\n",
    "            selected_probs.append(prob)\n",
    "            selected_points.append(point)\n",
    "\n",
    "        if batch_mode:\n",
    "            selected_boxes = np.array(selected_boxes)\n",
    "            selected_probs = np.array(selected_probs)\n",
    "            selected_points = np.array(selected_points)\n",
    "        else:\n",
    "            selected_boxes = selected_boxes[0]\n",
    "            selected_probs = selected_probs[0][0]\n",
    "            selected_points = selected_points[0]\n",
    "\n",
    "        return selected_boxes, selected_probs, selected_points\n",
    "\n",
    "    def extract(self, img, batch_boxes, save_path):\n",
    "        # Determine if a batch or single image was passed\n",
    "        batch_mode = True\n",
    "        if (\n",
    "                not isinstance(img, (list, tuple)) and\n",
    "                not (isinstance(img, np.ndarray) and len(img.shape) == 4) and\n",
    "                not (isinstance(img, torch.Tensor) and len(img.shape) == 4)\n",
    "        ):\n",
    "            img = [img]\n",
    "            batch_boxes = [batch_boxes]\n",
    "            batch_mode = False\n",
    "\n",
    "        # Parse save path(s)\n",
    "        if save_path is not None:\n",
    "            if isinstance(save_path, str):\n",
    "                save_path = [save_path]\n",
    "        else:\n",
    "            save_path = [None for _ in range(len(img))]\n",
    "\n",
    "        # Process all bounding boxes\n",
    "        faces = []\n",
    "        for im, box_im, path_im in zip(img, batch_boxes, save_path):\n",
    "            if box_im is None:\n",
    "                faces.append(None)\n",
    "                continue\n",
    "\n",
    "            if not self.keep_all:\n",
    "                box_im = box_im[[0]]\n",
    "\n",
    "            faces_im = []\n",
    "            for i, box in enumerate(box_im):\n",
    "                face_path = path_im\n",
    "                if path_im is not None and i > 0:\n",
    "                    save_name, ext = os.path.splitext(path_im)\n",
    "                    face_path = save_name + '_' + str(i + 1) + ext\n",
    "\n",
    "                face = extract_face(im, box, self.image_size, self.margin, face_path)\n",
    "                if self.post_process:\n",
    "                    face = fixed_image_standardization(face)\n",
    "                faces_im.append(face)\n",
    "\n",
    "            if self.keep_all:\n",
    "                faces_im = torch.stack(faces_im)\n",
    "            else:\n",
    "                faces_im = faces_im[0]\n",
    "\n",
    "            faces.append(faces_im)\n",
    "\n",
    "        if not batch_mode:\n",
    "            faces = faces[0]\n",
    "\n",
    "        return faces\n",
    "\n",
    "\n",
    "def fixed_image_standardization(image_tensor):\n",
    "    processed_tensor = (image_tensor - 127.5) / 128.0\n",
    "    return processed_tensor\n",
    "\n",
    "\n",
    "def prewhiten(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    std_adj = std.clamp(min=1.0/(float(x.numel())**0.5))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
