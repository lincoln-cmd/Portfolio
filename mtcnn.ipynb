{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/timesler/facenet-pytorch/blob/master/models/mtcnn.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# OpenCV is optional, but required if using numpy arrays instead of PIL\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def fixed_batch_process(im_data, model):\n",
    "    batch_size = 512\n",
    "    out = []\n",
    "    for i in range(0, len(im_data), batch_size):\n",
    "        batch = im_data[i:(i+batch_size)]\n",
    "        out.append(model(batch))\n",
    "\n",
    "    return tuple(torch.cat(v, dim=0) for v in zip(*out))\n",
    "\n",
    "def detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):\n",
    "    if isinstance(imgs, (np.ndarray, torch.Tensor)):\n",
    "        if isinstance(imgs,np.ndarray):\n",
    "            imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "        if isinstance(imgs,torch.Tensor):\n",
    "            imgs = torch.as_tensor(imgs, device=device)\n",
    "\n",
    "        if len(imgs.shape) == 3:\n",
    "            imgs = imgs.unsqueeze(0)\n",
    "    else:\n",
    "        if not isinstance(imgs, (list, tuple)):\n",
    "            imgs = [imgs]\n",
    "        if any(img.size != imgs[0].size for img in imgs):\n",
    "            raise Exception(\"MTCNN batch processing only compatible with equal-dimension images.\")\n",
    "        imgs = np.stack([np.uint8(img) for img in imgs])\n",
    "        imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "    \n",
    "\n",
    "    model_dtype = next(pnet.parameters()).dtype\n",
    "    imgs = imgs.permute(0, 3, 1, 2).type(model_dtype)\n",
    "\n",
    "    batch_size = len(imgs)\n",
    "    h, w = imgs.shape[2:4]\n",
    "    m = 12.0 / minsize\n",
    "    minl = min(h, w)\n",
    "    minl = minl * m\n",
    "\n",
    "    # Create scale pyramid\n",
    "    scale_i = m\n",
    "    scales = []\n",
    "    while minl >= 12:\n",
    "        scales.append(scale_i)\n",
    "        scale_i = scale_i * factor\n",
    "        minl = minl * factor\n",
    "\n",
    "    # First stage\n",
    "    boxes = []\n",
    "    image_inds = []\n",
    "\n",
    "    scale_picks = []\n",
    "\n",
    "    all_i = 0\n",
    "    offset = 0\n",
    "    for scale in scales:\n",
    "        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        reg, probs = pnet(im_data)\n",
    "    \n",
    "        boxes_scale, image_inds_scale = generateBoundingBox(reg, probs[:, 1], scale, threshold[0])\n",
    "        boxes.append(boxes_scale)\n",
    "        image_inds.append(image_inds_scale)\n",
    "\n",
    "        pick = batched_nms(boxes_scale[:, :4], boxes_scale[:, 4], image_inds_scale, 0.5)\n",
    "        scale_picks.append(pick + offset)\n",
    "        offset += boxes_scale.shape[0]\n",
    "\n",
    "    boxes = torch.cat(boxes, dim=0)\n",
    "    image_inds = torch.cat(image_inds, dim=0)\n",
    "\n",
    "    scale_picks = torch.cat(scale_picks, dim=0)\n",
    "\n",
    "    # NMS within each scale + image\n",
    "    boxes, image_inds = boxes[scale_picks], image_inds[scale_picks]\n",
    "\n",
    "\n",
    "    # NMS within each image\n",
    "    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "    boxes, image_inds = boxes[pick], image_inds[pick]\n",
    "\n",
    "    regw = boxes[:, 2] - boxes[:, 0]\n",
    "    regh = boxes[:, 3] - boxes[:, 1]\n",
    "    qq1 = boxes[:, 0] + boxes[:, 5] * regw\n",
    "    qq2 = boxes[:, 1] + boxes[:, 6] * regh\n",
    "    qq3 = boxes[:, 2] + boxes[:, 7] * regw\n",
    "    qq4 = boxes[:, 3] + boxes[:, 8] * regh\n",
    "    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)\n",
    "    boxes = rerec(boxes)\n",
    "    y, ey, x, ex = pad(boxes, w, h)\n",
    "    \n",
    "    # Second stage\n",
    "    if len(boxes) > 0:\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (24, 24)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "\n",
    "        # This is equivalent to out = rnet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, rnet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        score = out1[1, :]\n",
    "        ipass = score > threshold[1]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        # NMS within each image\n",
    "        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]\n",
    "        boxes = bbreg(boxes, mv)\n",
    "        boxes = rerec(boxes)\n",
    "\n",
    "    # Third stage\n",
    "    points = torch.zeros(0, 5, 2, device=device)\n",
    "    if len(boxes) > 0:\n",
    "        y, ey, x, ex = pad(boxes, w, h)\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (48, 48)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        \n",
    "        # This is equivalent to out = onet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, onet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        out2 = out[2].permute(1, 0)\n",
    "        score = out2[1, :]\n",
    "        points = out1\n",
    "        ipass = score > threshold[2]\n",
    "        points = points[:, ipass]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        w_i = boxes[:, 2] - boxes[:, 0] + 1\n",
    "        h_i = boxes[:, 3] - boxes[:, 1] + 1\n",
    "        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1\n",
    "        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1\n",
    "        points = torch.stack((points_x, points_y)).permute(2, 1, 0)\n",
    "        boxes = bbreg(boxes, mv)\n",
    "\n",
    "        # NMS within each image using \"Min\" strategy\n",
    "        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, 'Min')\n",
    "        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]\n",
    "\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    points = points.cpu().numpy()\n",
    "\n",
    "    image_inds = image_inds.cpu()\n",
    "\n",
    "    batch_boxes = []\n",
    "    batch_points = []\n",
    "    for b_i in range(batch_size):\n",
    "        b_i_inds = np.where(image_inds == b_i)\n",
    "        batch_boxes.append(boxes[b_i_inds].copy())\n",
    "        batch_points.append(points[b_i_inds].copy())\n",
    "\n",
    "    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
    "\n",
    "    return batch_boxes, batch_points\n",
    "\n",
    "\n",
    "def bbreg(boundingbox, reg):\n",
    "    if reg.shape[1] == 1:\n",
    "        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))\n",
    "\n",
    "    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n",
    "    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n",
    "    b1 = boundingbox[:, 0] + reg[:, 0] * w\n",
    "    b2 = boundingbox[:, 1] + reg[:, 1] * h\n",
    "    b3 = boundingbox[:, 2] + reg[:, 2] * w\n",
    "    b4 = boundingbox[:, 3] + reg[:, 3] * h\n",
    "    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)\n",
    "\n",
    "    return boundingbox\n",
    "\n",
    "\n",
    "def generateBoundingBox(reg, probs, scale, thresh):\n",
    "    stride = 2\n",
    "    cellsize = 12\n",
    "\n",
    "    reg = reg.permute(1, 0, 2, 3)\n",
    "\n",
    "    mask = probs >= thresh\n",
    "    mask_inds = mask.nonzero()\n",
    "    image_inds = mask_inds[:, 0]\n",
    "    score = probs[mask]\n",
    "    reg = reg[:, mask].permute(1, 0)\n",
    "    bb = mask_inds[:, 1:].type(reg.dtype).flip(1)\n",
    "    q1 = ((stride * bb + 1) / scale).floor()\n",
    "    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()\n",
    "    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)\n",
    "    return boundingbox, image_inds\n",
    "\n",
    "\n",
    "def nms_numpy(boxes, scores, threshold, method):\n",
    "    if boxes.size == 0:\n",
    "        return np.empty((0, 3))\n",
    "\n",
    "    x1 = boxes[:, 0].copy()\n",
    "    y1 = boxes[:, 1].copy()\n",
    "    x2 = boxes[:, 2].copy()\n",
    "    y2 = boxes[:, 3].copy()\n",
    "    s = scores\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    I = np.argsort(s)\n",
    "    pick = np.zeros_like(s, dtype=np.int16)\n",
    "    counter = 0\n",
    "    while I.size > 0:\n",
    "        i = I[-1]\n",
    "        pick[counter] = i\n",
    "        counter += 1\n",
    "        idx = I[0:-1]\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idx]).copy()\n",
    "        yy1 = np.maximum(y1[i], y1[idx]).copy()\n",
    "        xx2 = np.minimum(x2[i], x2[idx]).copy()\n",
    "        yy2 = np.minimum(y2[i], y2[idx]).copy()\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1).copy()\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1).copy()\n",
    "\n",
    "        inter = w * h\n",
    "        if method is \"Min\":\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "        I = I[np.where(o <= threshold)]\n",
    "\n",
    "    pick = pick[:counter].copy()\n",
    "    return pick\n",
    "\n",
    "\n",
    "def batched_nms_numpy(boxes, scores, idxs, threshold, method):\n",
    "    device = boxes.device\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=device)\n",
    "    # strategy: in order to perform NMS independently per class.\n",
    "    # we add an offset to all the boxes. The offset is dependent\n",
    "    # only on the class idx, and is large enough so that boxes\n",
    "    # from different classes do not overlap\n",
    "    max_coordinate = boxes.max()\n",
    "    offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "    boxes_for_nms = boxes + offsets[:, None]\n",
    "    boxes_for_nms = boxes_for_nms.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "    keep = nms_numpy(boxes_for_nms, scores, threshold, method)\n",
    "    return torch.as_tensor(keep, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "def pad(boxes, w, h):\n",
    "    boxes = boxes.trunc().int().cpu().numpy()\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    ex = boxes[:, 2]\n",
    "    ey = boxes[:, 3]\n",
    "\n",
    "    x[x < 1] = 1\n",
    "    y[y < 1] = 1\n",
    "    ex[ex > w] = w\n",
    "    ey[ey > h] = h\n",
    "\n",
    "    return y, ey, x, ex\n",
    "\n",
    "\n",
    "def rerec(bboxA):\n",
    "    h = bboxA[:, 3] - bboxA[:, 1]\n",
    "    w = bboxA[:, 2] - bboxA[:, 0]\n",
    "    \n",
    "    l = torch.max(w, h)\n",
    "    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n",
    "    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n",
    "    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)\n",
    "\n",
    "    return bboxA\n",
    "\n",
    "\n",
    "def imresample(img, sz):\n",
    "    im_data = interpolate(img, size=sz, mode=\"area\")\n",
    "    return im_data\n",
    "\n",
    "\n",
    "def crop_resize(img, box, image_size):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = cv2.resize(\n",
    "            img,\n",
    "            (image_size, image_size),\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        ).copy()\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = imresample(\n",
    "            img.permute(2, 0, 1).unsqueeze(0).float(),\n",
    "            (image_size, image_size)\n",
    "        ).byte().squeeze(0).permute(1, 2, 0)\n",
    "    else:\n",
    "        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_img(img, path):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    else:\n",
    "        img.save(path)\n",
    "\n",
    "\n",
    "def get_size(img):\n",
    "    if isinstance(img, (np.ndarray, torch.Tensor)):\n",
    "        return img.shape[1::-1]\n",
    "    else:\n",
    "        return img.size\n",
    "\n",
    "\n",
    "def extract_face(img, box, image_size=160, margin=0, save_path=None):\n",
    "    \"\"\"Extract face + margin from PIL Image given bounding box.\n",
    "    \n",
    "    Arguments:\n",
    "        img {PIL.Image} -- A PIL Image.\n",
    "        box {numpy.ndarray} -- Four-element bounding box.\n",
    "        image_size {int} -- Output image size in pixels. The image will be square.\n",
    "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
    "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
    "            repo, which applies the margin to the original image before resizing, making the margin\n",
    "            dependent on the original image size.\n",
    "        save_path {str} -- Save path for extracted face image. (default: {None})\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- tensor representing the extracted face.\n",
    "    \"\"\"\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (image_size - margin),\n",
    "        margin * (box[3] - box[1]) / (image_size - margin),\n",
    "    ]\n",
    "    raw_image_size = get_size(img)\n",
    "    box = [\n",
    "        int(max(box[0] - margin[0] / 2, 0)),\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\n",
    "        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\n",
    "    ]\n",
    "\n",
    "    face = crop_resize(img, box, image_size)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path) + \"/\", exist_ok=True)\n",
    "        save_img(face, save_path)\n",
    "\n",
    "    face = F.to_tensor(np.float32(face))\n",
    "\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if available\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        # fake tqdm if it's not installed\n",
    "        class tqdm(object):  # type: ignore\n",
    "\n",
    "            def __init__(self, total=None, disable=False,\n",
    "                         unit=None, unit_scale=None, unit_divisor=None):\n",
    "                self.total = total\n",
    "                self.disable = disable\n",
    "                self.n = 0\n",
    "                # ignore unit, unit_scale, unit_divisor; they're just for real tqdm\n",
    "\n",
    "            def update(self, n):\n",
    "                if self.disable:\n",
    "                    return\n",
    "\n",
    "                self.n += n\n",
    "                if self.total is None:\n",
    "                    sys.stderr.write(\"\\r{0:.1f} bytes\".format(self.n))\n",
    "                else:\n",
    "                    sys.stderr.write(\"\\r{0:.1f}%\".format(100 * self.n / float(self.total)))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            def __enter__(self):\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "                if self.disable:\n",
    "                    return\n",
    "\n",
    "                sys.stderr.write('\\n')\n",
    "\n",
    "\n",
    "def download_url_to_file(url, dst, hash_prefix=None, progress=True):\n",
    "    r\"\"\"Download object at the given URL to a local path.\n",
    "    Args:\n",
    "        url (string): URL of the object to download\n",
    "        dst (string): Full path where object will be saved, e.g. `/tmp/temporary_file`\n",
    "        hash_prefix (string, optional): If not None, the SHA256 downloaded file should start with `hash_prefix`.\n",
    "            Default: None\n",
    "        progress (bool, optional): whether or not to display a progress bar to stderr\n",
    "            Default: True\n",
    "    Example:\n",
    "        >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n",
    "    \"\"\"\n",
    "    file_size = None\n",
    "    # We use a different API for python2 since urllib(2) doesn't recognize the CA\n",
    "    # certificates in older Python\n",
    "    req = Request(url, headers={\"User-Agent\": \"torch.hub\"})\n",
    "    u = urlopen(req)\n",
    "    meta = u.info()\n",
    "    if hasattr(meta, 'getheaders'):\n",
    "        content_length = meta.getheaders(\"Content-Length\")\n",
    "    else:\n",
    "        content_length = meta.get_all(\"Content-Length\")\n",
    "    if content_length is not None and len(content_length) > 0:\n",
    "        file_size = int(content_length[0])\n",
    "\n",
    "    # We deliberately save it in a temp file and move it after\n",
    "    # download is complete. This prevents a local working checkpoint\n",
    "    # being overridden by a broken download.\n",
    "    dst = os.path.expanduser(dst)\n",
    "    dst_dir = os.path.dirname(dst)\n",
    "    f = tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)\n",
    "\n",
    "    try:\n",
    "        if hash_prefix is not None:\n",
    "            sha256 = hashlib.sha256()\n",
    "        with tqdm(total=file_size, disable=not progress,\n",
    "                  unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "            while True:\n",
    "                buffer = u.read(8192)\n",
    "                if len(buffer) == 0:\n",
    "                    break\n",
    "                f.write(buffer)\n",
    "                if hash_prefix is not None:\n",
    "                    sha256.update(buffer)\n",
    "                pbar.update(len(buffer))\n",
    "\n",
    "        f.close()\n",
    "        if hash_prefix is not None:\n",
    "            digest = sha256.hexdigest()\n",
    "            if digest[:len(hash_prefix)] != hash_prefix:\n",
    "                raise RuntimeError('invalid hash value (expected \"{}\", got \"{}\")'\n",
    "                                   .format(hash_prefix, digest))\n",
    "        shutil.move(f.name, dst)\n",
    "    finally:\n",
    "        f.close()\n",
    "        if os.path.exists(f.name):\n",
    "            os.remove(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dependencies.facenet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b209bfa40634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfacenet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfacenet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfacenet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minception_resnet_v1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_mdl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfacenet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dependencies.facenet'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import json\n",
    "import os, sys\n",
    "\n",
    "from dependencies.facenet.src import facenet\n",
    "from dependencies.facenet.src.models import inception_resnet_v1 as tf_mdl\n",
    "from dependencies.facenet.src.align import detect_face\n",
    "\n",
    "from models.inception_resnet_v1 import InceptionResnetV1\n",
    "from models.mtcnn import PNet, RNet, ONet\n",
    "\n",
    "\n",
    "def import_tf_params(tf_mdl_dir, sess):\n",
    "    \"\"\"Import tensorflow model from save directory.\n",
    "    \n",
    "    Arguments:\n",
    "        tf_mdl_dir {str} -- Location of protobuf, checkpoint, meta files.\n",
    "        sess {tensorflow.Session} -- Tensorflow session object.\n",
    "    \n",
    "    Returns:\n",
    "        (list, list, list) -- Tuple of lists containing the layer names,\n",
    "            parameter arrays as numpy ndarrays, parameter shapes.\n",
    "    \"\"\"\n",
    "    print('\\nLoading tensorflow model\\n')\n",
    "    if callable(tf_mdl_dir):\n",
    "        tf_mdl_dir(sess)\n",
    "    else:\n",
    "        facenet.load_model(tf_mdl_dir)\n",
    "\n",
    "    print('\\nGetting model weights\\n')\n",
    "    tf_layers = tf.trainable_variables()\n",
    "    tf_params = sess.run(tf_layers)\n",
    "\n",
    "    tf_shapes = [p.shape for p in tf_params]\n",
    "    tf_layers = [l.name for l in tf_layers]\n",
    "\n",
    "    if not callable(tf_mdl_dir):\n",
    "        path = os.path.join(tf_mdl_dir, 'layer_description.json')\n",
    "    else:\n",
    "        path = 'data/layer_description.json'\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump({l: s for l, s in zip(tf_layers, tf_shapes)}, f)\n",
    "\n",
    "    return tf_layers, tf_params, tf_shapes\n",
    "\n",
    "\n",
    "def get_layer_indices(layer_lookup, tf_layers):\n",
    "    \"\"\"Giving a lookup of model layer attribute names and tensorflow variable names,\n",
    "    find matching parameters.\n",
    "    \n",
    "    Arguments:\n",
    "        layer_lookup {dict} -- Dictionary mapping pytorch attribute names to (partial)\n",
    "            tensorflow variable names. Expects dict of the form {'attr': ['tf_name', ...]}\n",
    "            where the '...'s are ignored.\n",
    "        tf_layers {list} -- List of tensorflow variable names.\n",
    "    \n",
    "    Returns:\n",
    "        list -- The input dictionary with the list of matching inds appended to each item.\n",
    "    \"\"\"\n",
    "    layer_inds = {}\n",
    "    for name, value in layer_lookup.items():\n",
    "        layer_inds[name] = value + [[i for i, n in enumerate(tf_layers) if value[0] in n]]\n",
    "    return layer_inds\n",
    "\n",
    "\n",
    "def load_tf_batchNorm(weights, layer):\n",
    "    \"\"\"Load tensorflow weights into nn.BatchNorm object.\n",
    "    \n",
    "    Arguments:\n",
    "        weights {list} -- Tensorflow parameters.\n",
    "        layer {torch.nn.Module} -- nn.BatchNorm.\n",
    "    \"\"\"\n",
    "    layer.bias.data = torch.tensor(weights[0]).view(layer.bias.data.shape)\n",
    "    layer.weight.data = torch.ones_like(layer.weight.data)\n",
    "    layer.running_mean = torch.tensor(weights[1]).view(layer.running_mean.shape)\n",
    "    layer.running_var = torch.tensor(weights[2]).view(layer.running_var.shape)\n",
    "\n",
    "\n",
    "def load_tf_conv2d(weights, layer, transpose=False):\n",
    "    \"\"\"Load tensorflow weights into nn.Conv2d object.\n",
    "    \n",
    "    Arguments:\n",
    "        weights {list} -- Tensorflow parameters.\n",
    "        layer {torch.nn.Module} -- nn.Conv2d.\n",
    "    \"\"\"\n",
    "    if isinstance(weights, list):\n",
    "        if len(weights) == 2:\n",
    "            layer.bias.data = (\n",
    "                torch.tensor(weights[1])\n",
    "                    .view(layer.bias.data.shape)\n",
    "            )\n",
    "        weights = weights[0]\n",
    "    \n",
    "    if transpose:\n",
    "        dim_order = (3, 2, 1, 0)\n",
    "    else:\n",
    "        dim_order = (3, 2, 0, 1)\n",
    "\n",
    "    layer.weight.data = (\n",
    "        torch.tensor(weights)\n",
    "            .permute(dim_order)\n",
    "            .view(layer.weight.data.shape)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_tf_conv2d_trans(weights, layer):\n",
    "    return load_tf_conv2d(weights, layer, transpose=True)\n",
    "\n",
    "\n",
    "def load_tf_basicConv2d(weights, layer):\n",
    "    \"\"\"Load tensorflow weights into grouped Conv2d+BatchNorm object.\n",
    "    \n",
    "    Arguments:\n",
    "        weights {list} -- Tensorflow parameters.\n",
    "        layer {torch.nn.Module} -- Object containing Conv2d+BatchNorm.\n",
    "    \"\"\"\n",
    "    load_tf_conv2d(weights[0], layer.conv)\n",
    "    load_tf_batchNorm(weights[1:], layer.bn)\n",
    "\n",
    "\n",
    "def load_tf_linear(weights, layer):\n",
    "    \"\"\"Load tensorflow weights into nn.Linear object.\n",
    "    \n",
    "    Arguments:\n",
    "        weights {list} -- Tensorflow parameters.\n",
    "        layer {torch.nn.Module} -- nn.Linear.\n",
    "    \"\"\"\n",
    "    if isinstance(weights, list):\n",
    "        if len(weights) == 2:\n",
    "            layer.bias.data = (\n",
    "                torch.tensor(weights[1])\n",
    "                    .view(layer.bias.data.shape)\n",
    "            )\n",
    "        weights = weights[0]\n",
    "    layer.weight.data = (\n",
    "        torch.tensor(weights)\n",
    "            .transpose(-1, 0)\n",
    "            .view(layer.weight.data.shape)\n",
    "    )\n",
    "\n",
    "\n",
    "# High-level parameter-loading functions:\n",
    "\n",
    "def load_tf_block35(weights, layer):\n",
    "    load_tf_basicConv2d(weights[:4], layer.branch0)\n",
    "    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n",
    "    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n",
    "    load_tf_basicConv2d(weights[12:16], layer.branch2[0])\n",
    "    load_tf_basicConv2d(weights[16:20], layer.branch2[1])\n",
    "    load_tf_basicConv2d(weights[20:24], layer.branch2[2])\n",
    "    load_tf_conv2d(weights[24:26], layer.conv2d)\n",
    "\n",
    "\n",
    "def load_tf_block17_8(weights, layer):\n",
    "    load_tf_basicConv2d(weights[:4], layer.branch0)\n",
    "    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n",
    "    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n",
    "    load_tf_basicConv2d(weights[12:16], layer.branch1[2])\n",
    "    load_tf_conv2d(weights[16:18], layer.conv2d)\n",
    "\n",
    "\n",
    "def load_tf_mixed6a(weights, layer):\n",
    "    if len(weights) != 16:\n",
    "        raise ValueError(f'Number of weight arrays ({len(weights)}) not equal to 16')\n",
    "    load_tf_basicConv2d(weights[:4], layer.branch0)\n",
    "    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n",
    "    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n",
    "    load_tf_basicConv2d(weights[12:16], layer.branch1[2])\n",
    "\n",
    "\n",
    "def load_tf_mixed7a(weights, layer):\n",
    "    if len(weights) != 28:\n",
    "        raise ValueError(f'Number of weight arrays ({len(weights)}) not equal to 28')\n",
    "    load_tf_basicConv2d(weights[:4], layer.branch0[0])\n",
    "    load_tf_basicConv2d(weights[4:8], layer.branch0[1])\n",
    "    load_tf_basicConv2d(weights[8:12], layer.branch1[0])\n",
    "    load_tf_basicConv2d(weights[12:16], layer.branch1[1])\n",
    "    load_tf_basicConv2d(weights[16:20], layer.branch2[0])\n",
    "    load_tf_basicConv2d(weights[20:24], layer.branch2[1])\n",
    "    load_tf_basicConv2d(weights[24:28], layer.branch2[2])\n",
    "\n",
    "\n",
    "def load_tf_repeats(weights, layer, rptlen, subfun):\n",
    "    if len(weights) % rptlen != 0:\n",
    "        raise ValueError(f'Number of weight arrays ({len(weights)}) not divisible by {rptlen}')\n",
    "    weights_split = [weights[i:i+rptlen] for i in range(0, len(weights), rptlen)]\n",
    "    for i, w in enumerate(weights_split):\n",
    "        subfun(w, getattr(layer, str(i)))\n",
    "\n",
    "\n",
    "def load_tf_repeat_1(weights, layer):\n",
    "    load_tf_repeats(weights, layer, 26, load_tf_block35)\n",
    "\n",
    "\n",
    "def load_tf_repeat_2(weights, layer):\n",
    "    load_tf_repeats(weights, layer, 18, load_tf_block17_8)\n",
    "\n",
    "\n",
    "def load_tf_repeat_3(weights, layer):\n",
    "    load_tf_repeats(weights, layer, 18, load_tf_block17_8)\n",
    "\n",
    "\n",
    "def test_loaded_params(mdl, tf_params, tf_layers):\n",
    "    \"\"\"Check each parameter in a pytorch model for an equivalent parameter\n",
    "    in a list of tensorflow variables.\n",
    "    \n",
    "    Arguments:\n",
    "        mdl {torch.nn.Module} -- Pytorch model.\n",
    "        tf_params {list} -- List of ndarrays representing tensorflow variables.\n",
    "        tf_layers {list} -- Corresponding list of tensorflow variable names.\n",
    "    \"\"\"\n",
    "    tf_means = torch.stack([torch.tensor(p).mean() for p in tf_params])\n",
    "    for name, param in mdl.named_parameters():\n",
    "        pt_mean = param.data.mean()\n",
    "        matching_inds = ((tf_means - pt_mean).abs() < 1e-8).nonzero()\n",
    "        print(f'{name} equivalent to {[tf_layers[i] for i in matching_inds]}')\n",
    "\n",
    "\n",
    "def compare_model_outputs(pt_mdl, sess, test_data):\n",
    "    \"\"\"Given some testing data, compare the output of pytorch and tensorflow models.\n",
    "    \n",
    "    Arguments:\n",
    "        pt_mdl {torch.nn.Module} -- Pytorch model.\n",
    "        sess {tensorflow.Session} -- Tensorflow session object.\n",
    "        test_data {torch.Tensor} -- Pytorch tensor.\n",
    "    \"\"\"\n",
    "    print('\\nPassing test data through TF model\\n')\n",
    "    if isinstance(sess, tf.Session):\n",
    "        images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "        embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        feed_dict = {images_placeholder: test_data.numpy(), phase_train_placeholder: False}\n",
    "        tf_output = torch.tensor(sess.run(embeddings, feed_dict=feed_dict))\n",
    "    else:\n",
    "        tf_output = sess(test_data)\n",
    "\n",
    "    print(tf_output)\n",
    "\n",
    "    print('\\nPassing test data through PT model\\n')\n",
    "    pt_output = pt_mdl(test_data.permute(0, 3, 1, 2))\n",
    "    print(pt_output)\n",
    "\n",
    "    distance = (tf_output - pt_output).norm()\n",
    "    print(f'\\nDistance {distance}\\n')\n",
    "\n",
    "\n",
    "def compare_mtcnn(pt_mdl, tf_fun, sess, ind, test_data):\n",
    "    tf_mdls = tf_fun(sess)\n",
    "    tf_mdl = tf_mdls[ind]\n",
    "\n",
    "    print('\\nPassing test data through TF model\\n')\n",
    "    tf_output = tf_mdl(test_data.numpy())\n",
    "    tf_output = [torch.tensor(out) for out in tf_output]\n",
    "    print('\\n'.join([str(o.view(-1)[:10]) for o in tf_output]))\n",
    "\n",
    "    print('\\nPassing test data through PT model\\n')\n",
    "    with torch.no_grad():\n",
    "        pt_output = pt_mdl(test_data.permute(0, 3, 2, 1))\n",
    "    pt_output = [torch.tensor(out) for out in pt_output]\n",
    "    for i in range(len(pt_output)):\n",
    "        if len(pt_output[i].shape) == 4:\n",
    "            pt_output[i] = pt_output[i].permute(0, 3, 2, 1).contiguous()\n",
    "    print('\\n'.join([str(o.view(-1)[:10]) for o in pt_output]))\n",
    "\n",
    "    distance = [(tf_o - pt_o).norm() for tf_o, pt_o in zip(tf_output, pt_output)]\n",
    "    print(f'\\nDistance {distance}\\n')\n",
    "\n",
    "\n",
    "def load_tf_model_weights(mdl, layer_lookup, tf_mdl_dir, is_resnet=True, arg_num=None):\n",
    "    \"\"\"Load tensorflow parameters into a pytorch model.\n",
    "    \n",
    "    Arguments:\n",
    "        mdl {torch.nn.Module} -- Pytorch model.\n",
    "        layer_lookup {[type]} -- Dictionary mapping pytorch attribute names to (partial)\n",
    "            tensorflow variable names, and a function suitable for loading weights.\n",
    "            Expects dict of the form {'attr': ['tf_name', function]}. \n",
    "        tf_mdl_dir {str} -- Location of protobuf, checkpoint, meta files.\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        tf_layers, tf_params, tf_shapes = import_tf_params(tf_mdl_dir, sess)\n",
    "        layer_info = get_layer_indices(layer_lookup, tf_layers)\n",
    "\n",
    "        for layer_name, info in layer_info.items():\n",
    "            print(f'Loading {info[0]}/* into {layer_name}')\n",
    "            weights = [tf_params[i] for i in info[2]]\n",
    "            layer = getattr(mdl, layer_name)\n",
    "            info[1](weights, layer)\n",
    "\n",
    "        test_loaded_params(mdl, tf_params, tf_layers)\n",
    "\n",
    "        if is_resnet:\n",
    "            compare_model_outputs(mdl, sess, torch.randn(5, 160, 160, 3).detach())\n",
    "\n",
    "\n",
    "def tensorflow2pytorch():\n",
    "    lookup_inception_resnet_v1 = {\n",
    "        'conv2d_1a': ['InceptionResnetV1/Conv2d_1a_3x3', load_tf_basicConv2d],\n",
    "        'conv2d_2a': ['InceptionResnetV1/Conv2d_2a_3x3', load_tf_basicConv2d],\n",
    "        'conv2d_2b': ['InceptionResnetV1/Conv2d_2b_3x3', load_tf_basicConv2d],\n",
    "        'conv2d_3b': ['InceptionResnetV1/Conv2d_3b_1x1', load_tf_basicConv2d],\n",
    "        'conv2d_4a': ['InceptionResnetV1/Conv2d_4a_3x3', load_tf_basicConv2d],\n",
    "        'conv2d_4b': ['InceptionResnetV1/Conv2d_4b_3x3', load_tf_basicConv2d],\n",
    "        'repeat_1': ['InceptionResnetV1/Repeat/block35', load_tf_repeat_1],\n",
    "        'mixed_6a': ['InceptionResnetV1/Mixed_6a', load_tf_mixed6a],\n",
    "        'repeat_2': ['InceptionResnetV1/Repeat_1/block17', load_tf_repeat_2],\n",
    "        'mixed_7a': ['InceptionResnetV1/Mixed_7a', load_tf_mixed7a],\n",
    "        'repeat_3': ['InceptionResnetV1/Repeat_2/block8', load_tf_repeat_3],\n",
    "        'block8': ['InceptionResnetV1/Block8', load_tf_block17_8],\n",
    "        'last_linear': ['InceptionResnetV1/Bottleneck/weights', load_tf_linear],\n",
    "        'last_bn': ['InceptionResnetV1/Bottleneck/BatchNorm', load_tf_batchNorm],\n",
    "        'logits': ['Logits', load_tf_linear],\n",
    "    }\n",
    "\n",
    "    print('\\nLoad VGGFace2-trained weights and save\\n')\n",
    "    mdl = InceptionResnetV1(num_classes=8631).eval()\n",
    "    tf_mdl_dir = 'data/20180402-114759'\n",
    "    data_name = 'vggface2'\n",
    "    load_tf_model_weights(mdl, lookup_inception_resnet_v1, tf_mdl_dir)\n",
    "    state_dict = mdl.state_dict()\n",
    "    torch.save(state_dict, f'{tf_mdl_dir}-{data_name}.pt')    \n",
    "    torch.save(\n",
    "        {\n",
    "            'logits.weight': state_dict['logits.weight'],\n",
    "            'logits.bias': state_dict['logits.bias'],\n",
    "        },\n",
    "        f'{tf_mdl_dir}-{data_name}-logits.pt'\n",
    "    )\n",
    "    state_dict.pop('logits.weight')\n",
    "    state_dict.pop('logits.bias')\n",
    "    torch.save(state_dict, f'{tf_mdl_dir}-{data_name}-features.pt')\n",
    "    \n",
    "    print('\\nLoad CASIA-Webface-trained weights and save\\n')\n",
    "    mdl = InceptionResnetV1(num_classes=10575).eval()\n",
    "    tf_mdl_dir = 'data/20180408-102900'\n",
    "    data_name = 'casia-webface'\n",
    "    load_tf_model_weights(mdl, lookup_inception_resnet_v1, tf_mdl_dir)\n",
    "    state_dict = mdl.state_dict()\n",
    "    torch.save(state_dict, f'{tf_mdl_dir}-{data_name}.pt')    \n",
    "    torch.save(\n",
    "        {\n",
    "            'logits.weight': state_dict['logits.weight'],\n",
    "            'logits.bias': state_dict['logits.bias'],\n",
    "        },\n",
    "        f'{tf_mdl_dir}-{data_name}-logits.pt'\n",
    "    )\n",
    "    state_dict.pop('logits.weight')\n",
    "    state_dict.pop('logits.bias')\n",
    "    torch.save(state_dict, f'{tf_mdl_dir}-{data_name}-features.pt')\n",
    "    \n",
    "    lookup_pnet = {\n",
    "        'conv1': ['pnet/conv1', load_tf_conv2d_trans],\n",
    "        'prelu1': ['pnet/PReLU1', load_tf_linear],\n",
    "        'conv2': ['pnet/conv2', load_tf_conv2d_trans],\n",
    "        'prelu2': ['pnet/PReLU2', load_tf_linear],\n",
    "        'conv3': ['pnet/conv3', load_tf_conv2d_trans],\n",
    "        'prelu3': ['pnet/PReLU3', load_tf_linear],\n",
    "        'conv4_1': ['pnet/conv4-1', load_tf_conv2d_trans],\n",
    "        'conv4_2': ['pnet/conv4-2', load_tf_conv2d_trans],\n",
    "    }\n",
    "    lookup_rnet = {\n",
    "        'conv1': ['rnet/conv1', load_tf_conv2d_trans],\n",
    "        'prelu1': ['rnet/prelu1', load_tf_linear],\n",
    "        'conv2': ['rnet/conv2', load_tf_conv2d_trans],\n",
    "        'prelu2': ['rnet/prelu2', load_tf_linear],\n",
    "        'conv3': ['rnet/conv3', load_tf_conv2d_trans],\n",
    "        'prelu3': ['rnet/prelu3', load_tf_linear],\n",
    "        'dense4': ['rnet/conv4', load_tf_linear],\n",
    "        'prelu4': ['rnet/prelu4', load_tf_linear],\n",
    "        'dense5_1': ['rnet/conv5-1', load_tf_linear],\n",
    "        'dense5_2': ['rnet/conv5-2', load_tf_linear],\n",
    "    }\n",
    "    lookup_onet = {\n",
    "        'conv1': ['onet/conv1', load_tf_conv2d_trans],\n",
    "        'prelu1': ['onet/prelu1', load_tf_linear],\n",
    "        'conv2': ['onet/conv2', load_tf_conv2d_trans],\n",
    "        'prelu2': ['onet/prelu2', load_tf_linear],\n",
    "        'conv3': ['onet/conv3', load_tf_conv2d_trans],\n",
    "        'prelu3': ['onet/prelu3', load_tf_linear],\n",
    "        'conv4': ['onet/conv4', load_tf_conv2d_trans],\n",
    "        'prelu4': ['onet/prelu4', load_tf_linear],\n",
    "        'dense5': ['onet/conv5', load_tf_linear],\n",
    "        'prelu5': ['onet/prelu5', load_tf_linear],\n",
    "        'dense6_1': ['onet/conv6-1', load_tf_linear],\n",
    "        'dense6_2': ['onet/conv6-2', load_tf_linear],\n",
    "        'dense6_3': ['onet/conv6-3', load_tf_linear],\n",
    "    }\n",
    "\n",
    "    print('\\nLoad PNet weights and save\\n')\n",
    "    tf_mdl_dir = lambda sess: detect_face.create_mtcnn(sess, None)\n",
    "    mdl = PNet()\n",
    "    data_name = 'pnet'\n",
    "    load_tf_model_weights(mdl, lookup_pnet, tf_mdl_dir, is_resnet=False, arg_num=0)\n",
    "    torch.save(mdl.state_dict(), f'data/{data_name}.pt')\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        compare_mtcnn(mdl, tf_mdl_dir, sess, 0, torch.randn(1, 256, 256, 3).detach())\n",
    "\n",
    "    print('\\nLoad RNet weights and save\\n')\n",
    "    mdl = RNet()\n",
    "    data_name = 'rnet'\n",
    "    load_tf_model_weights(mdl, lookup_rnet, tf_mdl_dir, is_resnet=False, arg_num=1)\n",
    "    torch.save(mdl.state_dict(), f'data/{data_name}.pt')\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        compare_mtcnn(mdl, tf_mdl_dir, sess, 1, torch.randn(1, 24, 24, 3).detach())\n",
    "\n",
    "    print('\\nLoad ONet weights and save\\n')\n",
    "    mdl = ONet()\n",
    "    data_name = 'onet'\n",
    "    load_tf_model_weights(mdl, lookup_onet, tf_mdl_dir, is_resnet=False, arg_num=2)\n",
    "    torch.save(mdl.state_dict(), f'data/{data_name}.pt')\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        compare_mtcnn(mdl, tf_mdl_dir, sess, 2, torch.randn(1, 48, 48, 3).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, mode, length, calculate_mean=False):\n",
    "        self.mode = mode\n",
    "        self.length = length\n",
    "        self.calculate_mean = calculate_mean\n",
    "        if self.calculate_mean:\n",
    "            self.fn = lambda x, i: x / (i + 1)\n",
    "        else:\n",
    "            self.fn = lambda x, i: x\n",
    "\n",
    "    def __call__(self, loss, metrics, i):\n",
    "        track_str = '\\r{} | {:5d}/{:<5d}| '.format(self.mode, i + 1, self.length)\n",
    "        loss_str = 'loss: {:9.4f} | '.format(self.fn(loss, i))\n",
    "        metric_str = ' | '.join('{}: {:9.4f}'.format(k, self.fn(v, i)) for k, v in metrics.items())\n",
    "        print(track_str + loss_str + metric_str + '   ', end='')\n",
    "        if i + 1 == self.length:\n",
    "            print('')\n",
    "\n",
    "\n",
    "class BatchTimer(object):\n",
    "    \"\"\"Batch timing class.\n",
    "    Use this class for tracking training and testing time/rate per batch or per sample.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        rate {bool} -- Whether to report a rate (batches or samples per second) or a time (seconds\n",
    "            per batch or sample). (default: {True})\n",
    "        per_sample {bool} -- Whether to report times or rates per sample or per batch.\n",
    "            (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rate=True, per_sample=True):\n",
    "        self.start = time.time()\n",
    "        self.end = None\n",
    "        self.rate = rate\n",
    "        self.per_sample = per_sample\n",
    "\n",
    "    def __call__(self, y_pred, y):\n",
    "        self.end = time.time()\n",
    "        elapsed = self.end - self.start\n",
    "        self.start = self.end\n",
    "        self.end = None\n",
    "\n",
    "        if self.per_sample:\n",
    "            elapsed /= len(y_pred)\n",
    "        if self.rate:\n",
    "            elapsed = 1 / elapsed\n",
    "\n",
    "        return torch.tensor(elapsed)\n",
    "\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "\n",
    "def pass_epoch(\n",
    "    model, loss_fn, loader, optimizer=None, scheduler=None,\n",
    "    batch_metrics={'time': BatchTimer()}, show_running=True,\n",
    "    device='cpu', writer=None\n",
    "):\n",
    "    \"\"\"Train or evaluate over a data epoch.\n",
    "    \n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Pytorch model.\n",
    "        loss_fn {callable} -- A function to compute (scalar) loss.\n",
    "        loader {torch.utils.data.DataLoader} -- A pytorch data loader.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        optimizer {torch.optim.Optimizer} -- A pytorch optimizer.\n",
    "        scheduler {torch.optim.lr_scheduler._LRScheduler} -- LR scheduler (default: {None})\n",
    "        batch_metrics {dict} -- Dictionary of metric functions to call on each batch. The default\n",
    "            is a simple timer. A progressive average of these metrics, along with the average\n",
    "            loss, is printed every batch. (default: {{'time': iter_timer()}})\n",
    "        show_running {bool} -- Whether or not to print losses and metrics for the current batch\n",
    "            or rolling averages. (default: {False})\n",
    "        device {str or torch.device} -- Device for pytorch to use. (default: {'cpu'})\n",
    "        writer {torch.utils.tensorboard.SummaryWriter} -- Tensorboard SummaryWriter. (default: {None})\n",
    "    \n",
    "    Returns:\n",
    "        tuple(torch.Tensor, dict) -- A tuple of the average loss and a dictionary of average\n",
    "            metric values across the epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    mode = 'Train' if model.training else 'Valid'\n",
    "    logger = Logger(mode, length=len(loader), calculate_mean=show_running)\n",
    "    loss = 0\n",
    "    metrics = {}\n",
    "\n",
    "    for i_batch, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss_batch = loss_fn(y_pred, y)\n",
    "\n",
    "        if model.training:\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        metrics_batch = {}\n",
    "        for metric_name, metric_fn in batch_metrics.items():\n",
    "            metrics_batch[metric_name] = metric_fn(y_pred, y).detach().cpu()\n",
    "            metrics[metric_name] = metrics.get(metric_name, 0) + metrics_batch[metric_name]\n",
    "            \n",
    "        if writer is not None and model.training:\n",
    "            if writer.iteration % writer.interval == 0:\n",
    "                writer.add_scalars('loss', {mode: loss_batch.detach().cpu()}, writer.iteration)\n",
    "                for metric_name, metric_batch in metrics_batch.items():\n",
    "                    writer.add_scalars(metric_name, {mode: metric_batch}, writer.iteration)\n",
    "            writer.iteration += 1\n",
    "        \n",
    "        loss_batch = loss_batch.detach().cpu()\n",
    "        loss += loss_batch\n",
    "        if show_running:\n",
    "            logger(loss, metrics, i_batch)\n",
    "        else:\n",
    "            logger(loss_batch, metrics_batch, i_batch)\n",
    "    \n",
    "    if model.training and scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    loss = loss / (i_batch + 1)\n",
    "    metrics = {k: v / (i_batch + 1) for k, v in metrics.items()}\n",
    "            \n",
    "    if writer is not None and not model.training:\n",
    "        writer.add_scalars('loss', {mode: loss.detach()}, writer.iteration)\n",
    "        for metric_name, metric in metrics.items():\n",
    "            writer.add_scalars(metric_name, {mode: metric})\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "\n",
    "def collate_pil(x): \n",
    "    out_x, out_y = [], [] \n",
    "    for xx, yy in x: \n",
    "        out_x.append(xx) \n",
    "        out_y.append(yy) \n",
    "    return out_x, out_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# $ cd /\n",
    "# python -m utils.detect_face\n",
    "# from .utils.detect_face import detect_face, extract_face\n",
    "# python3 -m utils.detect_face\n",
    "\n",
    "\n",
    "class PNet(nn.Module):\n",
    "    \"\"\"MTCNN PNet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(10)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.prelu3 = nn.PReLU(32)\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)\n",
    "        self.softmax4_1 = nn.Softmax(dim=1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/pnet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        a = self.conv4_1(x)\n",
    "        a = self.softmax4_1(a)\n",
    "        b = self.conv4_2(x)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class RNet(nn.Module):\n",
    "    \"\"\"MTCNN RNet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 28, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(28)\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(28, 48, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(48)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv3 = nn.Conv2d(48, 64, kernel_size=2)\n",
    "        self.prelu3 = nn.PReLU(64)\n",
    "        self.dense4 = nn.Linear(576, 128)\n",
    "        self.prelu4 = nn.PReLU(128)\n",
    "        self.dense5_1 = nn.Linear(128, 2)\n",
    "        self.softmax5_1 = nn.Softmax(dim=1)\n",
    "        self.dense5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/rnet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.dense4(x.view(x.shape[0], -1))\n",
    "        x = self.prelu4(x)\n",
    "        a = self.dense5_1(x)\n",
    "        a = self.softmax5_1(a)\n",
    "        b = self.dense5_2(x)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class ONet(nn.Module):\n",
    "    \"\"\"MTCNN ONet.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.prelu1 = nn.PReLU(32)\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.prelu2 = nn.PReLU(64)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.prelu3 = nn.PReLU(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.prelu4 = nn.PReLU(128)\n",
    "        self.dense5 = nn.Linear(1152, 256)\n",
    "        self.prelu5 = nn.PReLU(256)\n",
    "        self.dense6_1 = nn.Linear(256, 2)\n",
    "        self.softmax6_1 = nn.Softmax(dim=1)\n",
    "        self.dense6_2 = nn.Linear(256, 4)\n",
    "        self.dense6_3 = nn.Linear(256, 10)\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "        if pretrained:\n",
    "            state_dict_path = os.path.join(os.path.dirname(__file__), '../data/onet.pt')\n",
    "            state_dict = torch.load(state_dict_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.prelu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.dense5(x.view(x.shape[0], -1))\n",
    "        x = self.prelu5(x)\n",
    "        a = self.dense6_1(x)\n",
    "        a = self.softmax6_1(a)\n",
    "        b = self.dense6_2(x)\n",
    "        c = self.dense6_3(x)\n",
    "        return b, c, a\n",
    "\n",
    "\n",
    "class MTCNN(nn.Module):\n",
    "    \"\"\"MTCNN face detection module.\n",
    "    This class loads pretrained P-, R-, and O-nets and returns images cropped to include the face\n",
    "    only, given raw input images of one of the following types:\n",
    "        - PIL image or list of PIL images\n",
    "        - numpy.ndarray (uint8) representing either a single image (3D) or a batch of images (4D).\n",
    "    Cropped faces can optionally be saved to file\n",
    "    also.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\n",
    "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
    "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
    "            repo, which applies the margin to the original image before resizing, making the margin\n",
    "            dependent on the original image size (this is a bug in davidsandberg/facenet).\n",
    "            (default: {0})\n",
    "        min_face_size {int} -- Minimum face size to search for. (default: {20})\n",
    "        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\n",
    "        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\n",
    "        post_process {bool} -- Whether or not to post process images tensors before returning.\n",
    "            (default: {True})\n",
    "        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\n",
    "            If False, the face with the highest detection probability is returned.\n",
    "            (default: {True})\n",
    "        selection_method {string} -- Which heuristic to use for selection. Default None. If\n",
    "            specified, will override select_largest:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability selected\n",
    "                    \"center_weighted_size\": box size minus weighted squared offset from image center\n",
    "                (default: {None})\n",
    "        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\n",
    "            select_largest parameter. If a save_path is specified, the first face is saved to that\n",
    "            path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\n",
    "            (default: {False})\n",
    "        device {torch.device} -- The device on which to run neural net passes. Image tensors and\n",
    "            models are copied to this device before running forward passes. (default: {None})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, image_size=160, margin=0, min_face_size=20,\n",
    "        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "        select_largest=True, selection_method=None, keep_all=False, device=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.margin = margin\n",
    "        self.min_face_size = min_face_size\n",
    "        self.thresholds = thresholds\n",
    "        self.factor = factor\n",
    "        self.post_process = post_process\n",
    "        self.select_largest = select_largest\n",
    "        self.keep_all = keep_all\n",
    "        self.selection_method = selection_method\n",
    "\n",
    "        self.pnet = PNet()\n",
    "        self.rnet = RNet()\n",
    "        self.onet = ONet()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "            self.to(device)\n",
    "\n",
    "        if not self.selection_method:\n",
    "            self.selection_method = 'largest' if self.select_largest else 'probability'\n",
    "\n",
    "    def forward(self, img, save_path=None, return_prob=False):\n",
    "        \"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\n",
    "        detection and extraction of faces, returning tensors representing detected faces rather\n",
    "        than the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\n",
    "        \n",
    "        Arguments:\n",
    "            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            save_path {str} -- An optional save path for the cropped image. Note that when\n",
    "                self.post_process=True, although the returned tensor is post processed, the saved\n",
    "                face image is not, so it is a true representation of the face in the input image.\n",
    "                If `img` is a list of images, `save_path` should be a list of equal length.\n",
    "                (default: {None})\n",
    "            return_prob {bool} -- Whether or not to return the detection probability.\n",
    "                (default: {False})\n",
    "        \n",
    "        Returns:\n",
    "            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\n",
    "                with dimensions 3 x image_size x image_size. Optionally, the probability that a\n",
    "                face was detected. If self.keep_all is True, n detected faces are returned in an\n",
    "                n x 3 x image_size x image_size tensor with an optional list of detection\n",
    "                probabilities. If `img` is a list of images, the item(s) returned have an extra \n",
    "                dimension (batch) as the first dimension.\n",
    "        Example:\n",
    "        >>> from facenet_pytorch import MTCNN\n",
    "        >>> mtcnn = MTCNN()\n",
    "        >>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\n",
    "        \"\"\"\n",
    "\n",
    "        # Detect faces\n",
    "        batch_boxes, batch_probs, batch_points = self.detect(img, landmarks=True)\n",
    "        # Select faces\n",
    "        if not self.keep_all:\n",
    "            batch_boxes, batch_probs, batch_points = self.select_boxes(\n",
    "                batch_boxes, batch_probs, batch_points, img, method=self.selection_method\n",
    "            )\n",
    "        # Extract faces\n",
    "        faces = self.extract(img, batch_boxes, save_path)\n",
    "\n",
    "        if return_prob:\n",
    "            return faces, batch_probs\n",
    "        else:\n",
    "            return faces\n",
    "\n",
    "    def detect(self, img, landmarks=False):\n",
    "        \"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\n",
    "        This method is used by the forward method and is also useful for face detection tasks\n",
    "        that require lower-level handling of bounding boxes and facial landmarks (e.g., face\n",
    "        tracking). The functionality of the forward function can be emulated by using this method\n",
    "        followed by the extract_face() function.\n",
    "        \n",
    "        Arguments:\n",
    "            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        Keyword Arguments:\n",
    "            landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.\n",
    "                (default: {False})\n",
    "        \n",
    "        Returns:\n",
    "            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\n",
    "                Nx4 array of bounding boxes and a length N list of detection probabilities.\n",
    "                Returned boxes will be sorted in descending order by detection probability if\n",
    "                self.select_largest=False, otherwise the largest face will be returned first.\n",
    "                If `img` is a list of images, the items returned have an extra dimension\n",
    "                (batch) as the first dimension. Optionally, a third item, the facial landmarks,\n",
    "                are returned if `landmarks=True`.\n",
    "        Example:\n",
    "        >>> from PIL import Image, ImageDraw\n",
    "        >>> from facenet_pytorch import MTCNN, extract_face\n",
    "        >>> mtcnn = MTCNN(keep_all=True)\n",
    "        >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
    "        >>> # Draw boxes and save faces\n",
    "        >>> img_draw = img.copy()\n",
    "        >>> draw = ImageDraw.Draw(img_draw)\n",
    "        >>> for i, (box, point) in enumerate(zip(boxes, points)):\n",
    "        ...     draw.rectangle(box.tolist(), width=5)\n",
    "        ...     for p in point:\n",
    "        ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\n",
    "        ...     extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "        >>> img_draw.save('annotated_faces.png')\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_boxes, batch_points = detect_face(\n",
    "                img, self.min_face_size,\n",
    "                self.pnet, self.rnet, self.onet,\n",
    "                self.thresholds, self.factor,\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "        boxes, probs, points = [], [], []\n",
    "        for box, point in zip(batch_boxes, batch_points):\n",
    "            box = np.array(box)\n",
    "            point = np.array(point)\n",
    "            if len(box) == 0:\n",
    "                boxes.append(None)\n",
    "                probs.append([None])\n",
    "                points.append(None)\n",
    "            elif self.select_largest:\n",
    "                box_order = np.argsort((box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1]))[::-1]\n",
    "                box = box[box_order]\n",
    "                point = point[box_order]\n",
    "                boxes.append(box[:, :4])\n",
    "                probs.append(box[:, 4])\n",
    "                points.append(point)\n",
    "            else:\n",
    "                boxes.append(box[:, :4])\n",
    "                probs.append(box[:, 4])\n",
    "                points.append(point)\n",
    "        boxes = np.array(boxes)\n",
    "        probs = np.array(probs)\n",
    "        points = np.array(points)\n",
    "\n",
    "        if (\n",
    "            not isinstance(img, (list, tuple)) and \n",
    "            not (isinstance(img, np.ndarray) and len(img.shape) == 4) and\n",
    "            not (isinstance(img, torch.Tensor) and len(img.shape) == 4)\n",
    "        ):\n",
    "            boxes = boxes[0]\n",
    "            probs = probs[0]\n",
    "            points = points[0]\n",
    "\n",
    "        if landmarks:\n",
    "            return boxes, probs, points\n",
    "\n",
    "        return boxes, probs\n",
    "\n",
    "    def select_boxes(\n",
    "        self, all_boxes, all_probs, all_points, imgs, method='probability', threshold=0.9,\n",
    "        center_weight=2.0\n",
    "    ):\n",
    "        \"\"\"Selects a single box from multiple for a given image using one of multiple heuristics.\n",
    "        Arguments:\n",
    "                all_boxes {np.ndarray} -- Ix0 ndarray where each element is a Nx4 ndarry of\n",
    "                    bounding boxes for N detected faces in I images (output from self.detect).\n",
    "                all_probs {np.ndarray} -- Ix0 ndarray where each element is a Nx0 ndarry of\n",
    "                    probabilities for N detected faces in I images (output from self.detect).\n",
    "                all_points {np.ndarray} -- Ix0 ndarray where each element is a Nx5x2 array of\n",
    "                    points for N detected faces. (output from self.detect).\n",
    "                imgs {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.\n",
    "        Keyword Arguments:\n",
    "                method {str} -- Which heuristic to use for selection:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability selected\n",
    "                    \"center_weighted_size\": box size minus weighted squared offset from image center\n",
    "                    (default: {'probability'})\n",
    "                threshold {float} -- theshold for \"largest_over_threshold\" method. (default: {0.9})\n",
    "                center_weight {float} -- weight for squared offset in center weighted size method.\n",
    "                    (default: {2.0})\n",
    "        Returns:\n",
    "                tuple(numpy.ndarray, numpy.ndarray, numpy.ndarray) -- nx4 ndarray of bounding boxes\n",
    "                    for n images. Ix0 array of probabilities for each box, array of landmark points.\n",
    "        \"\"\"\n",
    "\n",
    "        #copying batch detection from extract, but would be easier to ensure detect creates consistent output.\n",
    "        batch_mode = True\n",
    "        if (\n",
    "                not isinstance(imgs, (list, tuple)) and\n",
    "                not (isinstance(imgs, np.ndarray) and len(imgs.shape) == 4) and\n",
    "                not (isinstance(imgs, torch.Tensor) and len(imgs.shape) == 4)\n",
    "        ):\n",
    "            imgs = [imgs]\n",
    "            all_boxes = [all_boxes]\n",
    "            all_probs = [all_probs]\n",
    "            all_points = [all_points]\n",
    "            batch_mode = False\n",
    "\n",
    "        selected_boxes, selected_probs, selected_points = [], [], []\n",
    "        for boxes, points, probs, img in zip(all_boxes, all_points, all_probs, imgs):\n",
    "            \n",
    "            if boxes is None:\n",
    "                selected_boxes.append(None)\n",
    "                selected_probs.append([None])\n",
    "                selected_points.append(None)\n",
    "                continue\n",
    "            \n",
    "            # If at least 1 box found\n",
    "            boxes = np.array(boxes)\n",
    "            probs = np.array(probs)\n",
    "            points = np.array(points)\n",
    "                \n",
    "            if method == 'largest':\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "            elif method == 'probability':\n",
    "                box_order = np.argsort(probs)[::-1]\n",
    "            elif method == 'center_weighted_size':\n",
    "                box_sizes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "                img_center = (img.width / 2, img.height/2)\n",
    "                box_centers = np.array(list(zip((boxes[:, 0] + boxes[:, 2]) / 2, (boxes[:, 1] + boxes[:, 3]) / 2)))\n",
    "                offsets = box_centers - img_center\n",
    "                offset_dist_squared = np.sum(np.power(offsets, 2.0), 1)\n",
    "                box_order = np.argsort(box_sizes - offset_dist_squared * center_weight)[::-1]\n",
    "            elif method == 'largest_over_threshold':\n",
    "                box_mask = probs > threshold\n",
    "                boxes = boxes[box_mask]\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "                if sum(box_mask) == 0:\n",
    "                    selected_boxes.append(None)\n",
    "                    selected_probs.append([None])\n",
    "                    selected_points.append(None)\n",
    "                    continue\n",
    "\n",
    "            box = boxes[box_order][[0]]\n",
    "            prob = probs[box_order][[0]]\n",
    "            point = points[box_order][[0]]\n",
    "            selected_boxes.append(box)\n",
    "            selected_probs.append(prob)\n",
    "            selected_points.append(point)\n",
    "\n",
    "        if batch_mode:\n",
    "            selected_boxes = np.array(selected_boxes)\n",
    "            selected_probs = np.array(selected_probs)\n",
    "            selected_points = np.array(selected_points)\n",
    "        else:\n",
    "            selected_boxes = selected_boxes[0]\n",
    "            selected_probs = selected_probs[0][0]\n",
    "            selected_points = selected_points[0]\n",
    "\n",
    "        return selected_boxes, selected_probs, selected_points\n",
    "\n",
    "    def extract(self, img, batch_boxes, save_path):\n",
    "        # Determine if a batch or single image was passed\n",
    "        batch_mode = True\n",
    "        if (\n",
    "                not isinstance(img, (list, tuple)) and\n",
    "                not (isinstance(img, np.ndarray) and len(img.shape) == 4) and\n",
    "                not (isinstance(img, torch.Tensor) and len(img.shape) == 4)\n",
    "        ):\n",
    "            img = [img]\n",
    "            batch_boxes = [batch_boxes]\n",
    "            batch_mode = False\n",
    "\n",
    "        # Parse save path(s)\n",
    "        if save_path is not None:\n",
    "            if isinstance(save_path, str):\n",
    "                save_path = [save_path]\n",
    "        else:\n",
    "            save_path = [None for _ in range(len(img))]\n",
    "\n",
    "        # Process all bounding boxes\n",
    "        faces = []\n",
    "        for im, box_im, path_im in zip(img, batch_boxes, save_path):\n",
    "            if box_im is None:\n",
    "                faces.append(None)\n",
    "                continue\n",
    "\n",
    "            if not self.keep_all:\n",
    "                box_im = box_im[[0]]\n",
    "\n",
    "            faces_im = []\n",
    "            for i, box in enumerate(box_im):\n",
    "                face_path = path_im\n",
    "                if path_im is not None and i > 0:\n",
    "                    save_name, ext = os.path.splitext(path_im)\n",
    "                    face_path = save_name + '_' + str(i + 1) + ext\n",
    "\n",
    "                face = extract_face(im, box, self.image_size, self.margin, face_path)\n",
    "                if self.post_process:\n",
    "                    face = fixed_image_standardization(face)\n",
    "                faces_im.append(face)\n",
    "\n",
    "            if self.keep_all:\n",
    "                faces_im = torch.stack(faces_im)\n",
    "            else:\n",
    "                faces_im = faces_im[0]\n",
    "\n",
    "            faces.append(faces_im)\n",
    "\n",
    "        if not batch_mode:\n",
    "            faces = faces[0]\n",
    "\n",
    "        return faces\n",
    "\n",
    "\n",
    "def fixed_image_standardization(image_tensor):\n",
    "    processed_tensor = (image_tensor - 127.5) / 128.0\n",
    "    return processed_tensor\n",
    "\n",
    "\n",
    "def prewhiten(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    std_adj = std.clamp(min=1.0/(float(x.numel())**0.5))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
