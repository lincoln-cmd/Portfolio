{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intellectual-communication",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 8450k    0 8450k    0     0  4090k      0 --:--:--  0:00:02 --:--:-- 4090k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  683k    0  683k    0     0   703k      0 --:--:-- --:--:-- --:--:--  702k\n",
      "100 3137k    0 3137k    0     0  2638k      0 --:--:--  0:00:01 --:--:-- 2638k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 4473k    0 4473k    0     0  2599k      0 --:--:--  0:00:01 --:--:-- 2597k\n",
      "100 6190k    0 6190k    0     0  3305k      0 --:--:--  0:00:01 --:--:-- 3303k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ./haarcascade_frontalface_default.xml https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "!curl -L -o ./haarcascade_eye.xml https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_eye.xml\n",
    "!curl -L -o ./haarcascade_frontalface_alt.xml https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "focused-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width : 1280.0, height : 720.0, fps : 34.97002997002997\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture('./debate.mp4')\n",
    "\n",
    "if video.isOpened() == False:\n",
    "    print('Cannot open this file')\n",
    "    exit()\n",
    "\n",
    "title = ['window']\n",
    "\n",
    "for i in title:\n",
    "    cv2.namedWindow(i)\n",
    "    \n",
    "width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "fps += 5.\n",
    "\n",
    "print('width : {0}, height : {1}, fps : {2}'.format(width, height, fps))\n",
    "\n",
    "filename = 'debate_cascade.avi'\n",
    "co = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "\n",
    "make = cv2.VideoWriter(filename, co, fps, (int(width), int(height)))\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + './haarcascade_frontalface_alt.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + './haarcascade_eye.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray_frame, (3,3), 0)\n",
    "    faces = face_cascade.detectMultiScale(blur, 1.1, 3)\n",
    "    \n",
    "    for x, y, w, h in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "        face = frame[x: x+w, y:y+h]\n",
    "        face_gray = gray_frame[x: x+w, y: y+h]\n",
    "        eye = eye_cascade.detectMultiScale(face_gray, 1.1, 3)\n",
    "        for ex, ey, ew, eh in eye:\n",
    "            cv2.rectangle(face, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 3)\n",
    "            \n",
    "    cv2.imshow(title[0], frame)\n",
    "    \n",
    "    make.write(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "video.release()\n",
    "make.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-backing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "banner-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7af0232dca6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatched_nms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.ops'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# OpenCV is optional, but required if using numpy arrays instead of PIL\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def fixed_batch_process(im_data, model):\n",
    "    batch_size = 512\n",
    "    out = []\n",
    "    for i in range(0, len(im_data), batch_size):\n",
    "        batch = im_data[i:(i+batch_size)]\n",
    "        out.append(model(batch))\n",
    "\n",
    "    return tuple(torch.cat(v, dim=0) for v in zip(*out))\n",
    "\n",
    "def detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):\n",
    "    if isinstance(imgs, (np.ndarray, torch.Tensor)):\n",
    "        if isinstance(imgs,np.ndarray):\n",
    "            imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "        if isinstance(imgs,torch.Tensor):\n",
    "            imgs = torch.as_tensor(imgs, device=device)\n",
    "\n",
    "        if len(imgs.shape) == 3:\n",
    "            imgs = imgs.unsqueeze(0)\n",
    "    else:\n",
    "        if not isinstance(imgs, (list, tuple)):\n",
    "            imgs = [imgs]\n",
    "        if any(img.size != imgs[0].size for img in imgs):\n",
    "            raise Exception(\"MTCNN batch processing only compatible with equal-dimension images.\")\n",
    "        imgs = np.stack([np.uint8(img) for img in imgs])\n",
    "        imgs = torch.as_tensor(imgs.copy(), device=device)\n",
    "\n",
    "    \n",
    "\n",
    "    model_dtype = next(pnet.parameters()).dtype\n",
    "    imgs = imgs.permute(0, 3, 1, 2).type(model_dtype)\n",
    "\n",
    "    batch_size = len(imgs)\n",
    "    h, w = imgs.shape[2:4]\n",
    "    m = 12.0 / minsize\n",
    "    minl = min(h, w)\n",
    "    minl = minl * m\n",
    "\n",
    "    # Create scale pyramid\n",
    "    scale_i = m\n",
    "    scales = []\n",
    "    while minl >= 12:\n",
    "        scales.append(scale_i)\n",
    "        scale_i = scale_i * factor\n",
    "        minl = minl * factor\n",
    "\n",
    "    # First stage\n",
    "    boxes = []\n",
    "    image_inds = []\n",
    "\n",
    "    scale_picks = []\n",
    "\n",
    "    all_i = 0\n",
    "    offset = 0\n",
    "    for scale in scales:\n",
    "        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        reg, probs = pnet(im_data)\n",
    "    \n",
    "        boxes_scale, image_inds_scale = generateBoundingBox(reg, probs[:, 1], scale, threshold[0])\n",
    "        boxes.append(boxes_scale)\n",
    "        image_inds.append(image_inds_scale)\n",
    "\n",
    "        pick = batched_nms(boxes_scale[:, :4], boxes_scale[:, 4], image_inds_scale, 0.5)\n",
    "        scale_picks.append(pick + offset)\n",
    "        offset += boxes_scale.shape[0]\n",
    "\n",
    "    boxes = torch.cat(boxes, dim=0)\n",
    "    image_inds = torch.cat(image_inds, dim=0)\n",
    "\n",
    "    scale_picks = torch.cat(scale_picks, dim=0)\n",
    "\n",
    "    # NMS within each scale + image\n",
    "    boxes, image_inds = boxes[scale_picks], image_inds[scale_picks]\n",
    "\n",
    "\n",
    "    # NMS within each image\n",
    "    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "    boxes, image_inds = boxes[pick], image_inds[pick]\n",
    "\n",
    "    regw = boxes[:, 2] - boxes[:, 0]\n",
    "    regh = boxes[:, 3] - boxes[:, 1]\n",
    "    qq1 = boxes[:, 0] + boxes[:, 5] * regw\n",
    "    qq2 = boxes[:, 1] + boxes[:, 6] * regh\n",
    "    qq3 = boxes[:, 2] + boxes[:, 7] * regw\n",
    "    qq4 = boxes[:, 3] + boxes[:, 8] * regh\n",
    "    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)\n",
    "    boxes = rerec(boxes)\n",
    "    y, ey, x, ex = pad(boxes, w, h)\n",
    "    \n",
    "    # Second stage\n",
    "    if len(boxes) > 0:\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (24, 24)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "\n",
    "        # This is equivalent to out = rnet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, rnet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        score = out1[1, :]\n",
    "        ipass = score > threshold[1]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        # NMS within each image\n",
    "        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]\n",
    "        boxes = bbreg(boxes, mv)\n",
    "        boxes = rerec(boxes)\n",
    "\n",
    "    # Third stage\n",
    "    points = torch.zeros(0, 5, 2, device=device)\n",
    "    if len(boxes) > 0:\n",
    "        y, ey, x, ex = pad(boxes, w, h)\n",
    "        im_data = []\n",
    "        for k in range(len(y)):\n",
    "            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n",
    "                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n",
    "                im_data.append(imresample(img_k, (48, 48)))\n",
    "        im_data = torch.cat(im_data, dim=0)\n",
    "        im_data = (im_data - 127.5) * 0.0078125\n",
    "        \n",
    "        # This is equivalent to out = onet(im_data) to avoid GPU out of memory.\n",
    "        out = fixed_batch_process(im_data, onet)\n",
    "\n",
    "        out0 = out[0].permute(1, 0)\n",
    "        out1 = out[1].permute(1, 0)\n",
    "        out2 = out[2].permute(1, 0)\n",
    "        score = out2[1, :]\n",
    "        points = out1\n",
    "        ipass = score > threshold[2]\n",
    "        points = points[:, ipass]\n",
    "        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n",
    "        image_inds = image_inds[ipass]\n",
    "        mv = out0[:, ipass].permute(1, 0)\n",
    "\n",
    "        w_i = boxes[:, 2] - boxes[:, 0] + 1\n",
    "        h_i = boxes[:, 3] - boxes[:, 1] + 1\n",
    "        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1\n",
    "        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1\n",
    "        points = torch.stack((points_x, points_y)).permute(2, 1, 0)\n",
    "        boxes = bbreg(boxes, mv)\n",
    "\n",
    "        # NMS within each image using \"Min\" strategy\n",
    "        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n",
    "        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, 'Min')\n",
    "        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]\n",
    "\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    points = points.cpu().numpy()\n",
    "\n",
    "    image_inds = image_inds.cpu()\n",
    "\n",
    "    batch_boxes = []\n",
    "    batch_points = []\n",
    "    for b_i in range(batch_size):\n",
    "        b_i_inds = np.where(image_inds == b_i)\n",
    "        batch_boxes.append(boxes[b_i_inds].copy())\n",
    "        batch_points.append(points[b_i_inds].copy())\n",
    "\n",
    "    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
    "\n",
    "    return batch_boxes, batch_points\n",
    "\n",
    "\n",
    "def bbreg(boundingbox, reg):\n",
    "    if reg.shape[1] == 1:\n",
    "        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))\n",
    "\n",
    "    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n",
    "    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n",
    "    b1 = boundingbox[:, 0] + reg[:, 0] * w\n",
    "    b2 = boundingbox[:, 1] + reg[:, 1] * h\n",
    "    b3 = boundingbox[:, 2] + reg[:, 2] * w\n",
    "    b4 = boundingbox[:, 3] + reg[:, 3] * h\n",
    "    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)\n",
    "\n",
    "    return boundingbox\n",
    "\n",
    "\n",
    "def generateBoundingBox(reg, probs, scale, thresh):\n",
    "    stride = 2\n",
    "    cellsize = 12\n",
    "\n",
    "    reg = reg.permute(1, 0, 2, 3)\n",
    "\n",
    "    mask = probs >= thresh\n",
    "    mask_inds = mask.nonzero()\n",
    "    image_inds = mask_inds[:, 0]\n",
    "    score = probs[mask]\n",
    "    reg = reg[:, mask].permute(1, 0)\n",
    "    bb = mask_inds[:, 1:].type(reg.dtype).flip(1)\n",
    "    q1 = ((stride * bb + 1) / scale).floor()\n",
    "    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()\n",
    "    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)\n",
    "    return boundingbox, image_inds\n",
    "\n",
    "\n",
    "def nms_numpy(boxes, scores, threshold, method):\n",
    "    if boxes.size == 0:\n",
    "        return np.empty((0, 3))\n",
    "\n",
    "    x1 = boxes[:, 0].copy()\n",
    "    y1 = boxes[:, 1].copy()\n",
    "    x2 = boxes[:, 2].copy()\n",
    "    y2 = boxes[:, 3].copy()\n",
    "    s = scores\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    I = np.argsort(s)\n",
    "    pick = np.zeros_like(s, dtype=np.int16)\n",
    "    counter = 0\n",
    "    while I.size > 0:\n",
    "        i = I[-1]\n",
    "        pick[counter] = i\n",
    "        counter += 1\n",
    "        idx = I[0:-1]\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idx]).copy()\n",
    "        yy1 = np.maximum(y1[i], y1[idx]).copy()\n",
    "        xx2 = np.minimum(x2[i], x2[idx]).copy()\n",
    "        yy2 = np.minimum(y2[i], y2[idx]).copy()\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1).copy()\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1).copy()\n",
    "\n",
    "        inter = w * h\n",
    "        if method is \"Min\":\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "        I = I[np.where(o <= threshold)]\n",
    "\n",
    "    pick = pick[:counter].copy()\n",
    "    return pick\n",
    "\n",
    "\n",
    "def batched_nms_numpy(boxes, scores, idxs, threshold, method):\n",
    "    device = boxes.device\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=device)\n",
    "    # strategy: in order to perform NMS independently per class.\n",
    "    # we add an offset to all the boxes. The offset is dependent\n",
    "    # only on the class idx, and is large enough so that boxes\n",
    "    # from different classes do not overlap\n",
    "    max_coordinate = boxes.max()\n",
    "    offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "    boxes_for_nms = boxes + offsets[:, None]\n",
    "    boxes_for_nms = boxes_for_nms.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "    keep = nms_numpy(boxes_for_nms, scores, threshold, method)\n",
    "    return torch.as_tensor(keep, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "def pad(boxes, w, h):\n",
    "    boxes = boxes.trunc().int().cpu().numpy()\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    ex = boxes[:, 2]\n",
    "    ey = boxes[:, 3]\n",
    "\n",
    "    x[x < 1] = 1\n",
    "    y[y < 1] = 1\n",
    "    ex[ex > w] = w\n",
    "    ey[ey > h] = h\n",
    "\n",
    "    return y, ey, x, ex\n",
    "\n",
    "\n",
    "def rerec(bboxA):\n",
    "    h = bboxA[:, 3] - bboxA[:, 1]\n",
    "    w = bboxA[:, 2] - bboxA[:, 0]\n",
    "    \n",
    "    l = torch.max(w, h)\n",
    "    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n",
    "    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n",
    "    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)\n",
    "\n",
    "    return bboxA\n",
    "\n",
    "\n",
    "def imresample(img, sz):\n",
    "    im_data = interpolate(img, size=sz, mode=\"area\")\n",
    "    return im_data\n",
    "\n",
    "\n",
    "def crop_resize(img, box, image_size):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = cv2.resize(\n",
    "            img,\n",
    "            (image_size, image_size),\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        ).copy()\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        img = img[box[1]:box[3], box[0]:box[2]]\n",
    "        out = imresample(\n",
    "            img.permute(2, 0, 1).unsqueeze(0).float(),\n",
    "            (image_size, image_size)\n",
    "        ).byte().squeeze(0).permute(1, 2, 0)\n",
    "    else:\n",
    "        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_img(img, path):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    else:\n",
    "        img.save(path)\n",
    "\n",
    "\n",
    "def get_size(img):\n",
    "    if isinstance(img, (np.ndarray, torch.Tensor)):\n",
    "        return img.shape[1::-1]\n",
    "    else:\n",
    "        return img.size\n",
    "\n",
    "\n",
    "def extract_face(img, box, image_size=160, margin=0, save_path=None):\n",
    "    \"\"\"Extract face + margin from PIL Image given bounding box.\n",
    "    \n",
    "    Arguments:\n",
    "        img {PIL.Image} -- A PIL Image.\n",
    "        box {numpy.ndarray} -- Four-element bounding box.\n",
    "        image_size {int} -- Output image size in pixels. The image will be square.\n",
    "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
    "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
    "            repo, which applies the margin to the original image before resizing, making the margin\n",
    "            dependent on the original image size.\n",
    "        save_path {str} -- Save path for extracted face image. (default: {None})\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor -- tensor representing the extracted face.\n",
    "    \"\"\"\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (image_size - margin),\n",
    "        margin * (box[3] - box[1]) / (image_size - margin),\n",
    "    ]\n",
    "    raw_image_size = get_size(img)\n",
    "    box = [\n",
    "        int(max(box[0] - margin[0] / 2, 0)),\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\n",
    "        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\n",
    "    ]\n",
    "\n",
    "    face = crop_resize(img, box, image_size)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path) + \"/\", exist_ok=True)\n",
    "        save_img(face, save_path)\n",
    "\n",
    "    face = F.to_tensor(np.float32(face))\n",
    "\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-consistency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
